{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RandomSCMProteomique",
      "provenance": [],
      "authorship_tag": "ABX9TyNQMCV1uaBeW0QpQiPgRLbv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samibahig/RecoverProject/blob/main/RandomSCMProteomique.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYYBb-wpqfLT",
        "outputId": "23f4dfd0-3205-4574-e78b-cbecccb43664"
      },
      "source": [
        "### Ici on download les donn√©es, on les nettoie et on les met sous forme de matrices: proteomics pour les features et metadata pour les labels, \n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle as pkl\n",
        "#print('\\nMETADATA :')\n",
        "data_path = ''\n",
        "metadata_filename = '/content/metadata.csv'\n",
        "meta_df = pd.read_csv(metadata_filename)\n",
        "print('-----------')\n",
        "print(meta_df)\n",
        "print('-----------')\n",
        "#print(meta_df.columns)\n",
        "meta_df.columns = ['#', 'plate', '-', 'symptoms'] + list(meta_df)[4:]\n",
        "#print(meta_df.columns)\n",
        "#print('available metadata :', list(meta_df))\n",
        "meta_idx = meta_df['ID'].to_list()\n",
        "meta_label = meta_df['symptoms'].to_list()\n",
        "#print('------------------')\n",
        "#print(list(zip(meta_idx, meta_label)))\n",
        "#print('------------------')\n",
        "meta_id_label_dict = {str(k): 1 if v=='S' else 0 for k, v in zip(meta_idx, meta_label)}\n",
        "\n",
        "#DF1 : proteomics\n",
        "#print('\\nPROEOMICS DATA :')\n",
        "proteomics_data_filename = '/content/proteomics.csv'\n",
        "\n",
        "dim_df = pd.read_csv(proteomics_data_filename, nrows=1)\n",
        "#print('--------')\n",
        "#print(dim_df)\n",
        "#print('--------')\n",
        "dim = len(list(dim_df))\n",
        "#print(dim)\n",
        "#print('------------')\n",
        "print('# of columns in source csv file :', dim)\n",
        "all_cols = [i for i in range(dim)]\n",
        "\n",
        "print('--------')\n",
        "feat_cols = all_cols[1:-4]\n",
        "print(feat_cols)\n",
        "print('--------')\n",
        "samplesidx_col = [0]\n",
        "\n",
        "feat_df = pd.read_csv(proteomics_data_filename, skiprows=4, nrows=1, dtype=str, usecols=feat_cols)\n",
        "features = list(feat_df)\n",
        "print('# of features : ', len(features))\n",
        "print('first feature :', features[0])\n",
        "print('last feature :', features[-1])\n",
        "\n",
        "idx_df = pd.read_csv(proteomics_data_filename, skiprows=6, index_col=0, skipfooter=4, usecols=[0], engine='python')\n",
        "idx = list(idx_df.index.values)\n",
        "print('# of idx : ', len(idx))\n",
        "print('first id :', idx[0])\n",
        "print('last id :', idx[-1])\n",
        "\n",
        "df1 = pd.read_csv(proteomics_data_filename, skiprows=6, dtype=np.float32, skipfooter=4, usecols=feat_cols, engine='python')\n",
        "assert df1.shape[0] == len(idx)\n",
        "assert df1.shape[1] == len(features)\n",
        "\n",
        "df1['idx'] = idx\n",
        "df1.set_index('idx', inplace=True)\n",
        "df1.columns = features\n",
        "print('# of Nan values :', df1.isna().sum().sum())\n",
        "\n",
        "#clean data of samples that are not in metadata :\n",
        "idx = df1.index.values\n",
        "y = []\n",
        "for k in range(len(idx)):\n",
        "    id = idx[k]\n",
        "    if id in meta_id_label_dict:\n",
        "        y.append(meta_id_label_dict[id])\n",
        "    else:\n",
        "        # we will not put this sample in the dataset\n",
        "        #print('sample to remove because of unknown label:', k, id)\n",
        "        y.append('to_remove')\n",
        "df1['label'] = y\n",
        "df1 = df1[df1.label != 'to_remove']\n",
        "\n",
        "#create X and y matrices for ML :\n",
        "y = list(df1['label'])\n",
        "del df1['label']\n",
        "print('---------')\n",
        "print(df1)\n",
        "print('---------')\n",
        "X = df1.to_numpy()\n",
        "print('proteomics data :')\n",
        "print('# of samples : ', df1.shape[0])\n",
        "print('# of features : ', df1.shape[1])\n",
        "print('labels:', list(dict.fromkeys(y)))\n",
        "\n",
        "\n",
        "## save X and y in pickles if you want :\n",
        "##data_name = 'recover_multiomics_'\n",
        "##feat_dict = {k: str(v) for k, v in zip(range(len(list(df))), list(df))}\n",
        "##with open(data_path + data_name + 'feat_dict', 'wb') as fo:\n",
        "##    pkl.dump(feat_dict, fo)\n",
        "##with open(data_path + data_name + 'X', 'wb') as fo:df\n",
        "##            pkl.dump(X, fo)\n",
        "##with open(data_path + data_name + 'y', 'wb') as fo:\n",
        "##            pkl.dump(y, fo)\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------\n",
            "    Unnamed: 0 Unnamed: 1  ... addition                                specify.1\n",
            "0            1    Plate 1  ...      NaN                                      NaN\n",
            "1            2    Plate 1  ...      5.0                                   Stress\n",
            "2            3    Plate 1  ...      6.0  back pain is worse, shortness of breath\n",
            "3            4    Plate 1  ...      2.0  Impression of a foreingh body in throat\n",
            "4            5    Plate 1  ...      2.0                                      NaN\n",
            "..         ...        ...  ...      ...                                      ...\n",
            "95         100    Plate 2  ...      NaN                                      NaN\n",
            "96         101    Plate 2  ...      NaN                                      NaN\n",
            "97         102    Plate 2  ...      NaN                                      NaN\n",
            "98         103    Plate 2  ...      NaN                                      NaN\n",
            "99         104    Plate 2  ...      NaN                                      NaN\n",
            "\n",
            "[100 rows x 46 columns]\n",
            "-----------\n",
            "# of columns in source csv file : 189\n",
            "--------\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184]\n",
            "--------\n",
            "# of features :  184\n",
            "first feature : Q96SB3\n",
            "last feature : P09603\n",
            "# of idx :  104\n",
            "first id : 5-139\n",
            "last id : 3-043\n",
            "# of Nan values : 0\n",
            "---------\n",
            "        Q96SB3   P16278   O75475   Q05516  ...  P05113.1   P00813   P01374    P09603\n",
            "idx                                        ...                                      \n",
            "5-139  1.61741  1.31276  3.20077  1.28369  ...   0.12752  5.46542  5.97368  10.40443\n",
            "1-039  2.79530  2.12398  2.90156  1.55239  ...   0.88631  5.24029  5.07646  10.14180\n",
            "1-062  1.69202  2.07015  2.07562  0.85847  ...   0.18121  5.34243  4.89149  10.02279\n",
            "1-040  1.62496  1.67346  2.30191  1.09831  ...   2.11383  5.54121  5.57492  10.13417\n",
            "2-044  1.69246  1.94635  1.87725  0.84233  ...   0.60991  4.98001  4.80933  10.04586\n",
            "...        ...      ...      ...      ...  ...       ...      ...      ...       ...\n",
            "1-073  1.40763  2.32194  2.53494  0.55014  ...   0.31641  5.31388  4.78352  10.50358\n",
            "2-108  1.73766  1.44918  2.23165  0.44999  ...   0.37494  5.44716  5.22955  10.13842\n",
            "5-114  2.94638  1.45486  3.31485  0.93572  ...   2.98474  5.34230  5.30634  10.16184\n",
            "5-075  2.09489  2.70359  2.17787  1.16964  ...   0.76538  5.14975  5.37112  10.40146\n",
            "3-043  2.23329  2.41564  3.45536  0.91918  ...   0.87126  5.34694  5.08457  10.35240\n",
            "\n",
            "[100 rows x 184 columns]\n",
            "---------\n",
            "proteomics data :\n",
            "# of samples :  100\n",
            "# of features :  184\n",
            "labels: [1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHfaxGhy05SD",
        "outputId": "652314e0-e514-43e6-9310-cf5e640bcfd1"
      },
      "source": [
        "pip install pyscm-ml   ### Ici on installe pyscm-ml librairie indispensable pour rouler l'algorithme RandomSCM"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyscm-ml in /usr/local/lib/python3.7/dist-packages (1.0.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pyscm-ml) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyscm-ml) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyscm-ml) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyscm-ml) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyscm-ml) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GjhW8n00Fdv"
      },
      "source": [
        "### Ici c'est l'algorithme SCM avec ses classes et toutes ses fonctions\n",
        "from sklearn.base import ClassifierMixin\n",
        "from sklearn.ensemble import BaseEnsemble\n",
        "from pyscm import SetCoveringMachineClassifier\n",
        "from pyscm.rules import DecisionStump\n",
        "\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted, check_random_state\n",
        "from sklearn.utils.random import sample_without_replacement\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numbers\n",
        "import itertools\n",
        "import numpy as np\n",
        "from warnings import warn\n",
        "from joblib import effective_n_jobs, Parallel, delayed\n",
        "\n",
        "MAX_INT = np.iinfo(np.int32).max\n",
        "def _parallel_build_estimators(idx, ensemble, p_of_estims, seeds, X, y, tiebreaker):\n",
        "    \"\"\"\n",
        "    Fit SCM estimators on subsamples of the training data\n",
        "    \"\"\"\n",
        "    estimators = []\n",
        "    estim_features = []\n",
        "    for k in idx:\n",
        "        p_param = p_of_estims[k] # p param for the classifier to fit\n",
        "        random_state = seeds[k]\n",
        "        estim = SetCoveringMachineClassifier(p=p_param, max_rules=ensemble.max_rules, model_type=ensemble.model_type, random_state=random_state)\n",
        "        feature_indices = sample_without_replacement(ensemble._pop_features, ensemble._max_features, random_state=random_state)\n",
        "        samples_indices = sample_without_replacement(ensemble._pop_samples, ensemble._max_samples, random_state=random_state)\n",
        "        Xk = (X[samples_indices])[:, feature_indices]\n",
        "        yk = y[samples_indices]\n",
        "        if len(list(set(yk))) < 2:\n",
        "            raise ValueError(\"One of the subsamples contains elements from only 1 class, try increase max_samples value\")\n",
        "        if tiebreaker is None:\n",
        "            estim.fit(Xk, yk)\n",
        "        else:\n",
        "            estim.fit(Xk, yk, tiebreaker)\n",
        "        estim_features.append(feature_indices)\n",
        "        estimators.append(estim)\n",
        "    return estimators, estim_features\n",
        "\n",
        "\n",
        "def _parallel_predict_proba(ensemble, X, idx, results):\n",
        "    \"\"\"\n",
        "    Compute predictions of SCM estimators\n",
        "    \"\"\"\n",
        "    for k in idx:\n",
        "        res = ensemble.estimators[k].predict(X[:, ensemble.estim_features[k]])\n",
        "        if ensemble.min_cq_combination:\n",
        "            res[res==0] = -1\n",
        "            results = results + ensemble.min_cq_weights[k]*res\n",
        "        else:\n",
        "            results = results + res\n",
        "    return results\n",
        "\n",
        "\n",
        "def _partition_estimators(n_estimators, n_jobs):\n",
        "    \"\"\"Private function used to partition estimators between jobs.\"\"\"\n",
        "    # Compute the number of jobs\n",
        "    n_jobs = min(effective_n_jobs(n_jobs), n_estimators)\n",
        "\n",
        "    # Partition estimators between jobs\n",
        "    n_estimators_per_job = np.full(n_jobs, n_estimators // n_jobs,\n",
        "                                   dtype=np.int)\n",
        "    n_estimators_per_job[:n_estimators % n_jobs] += 1\n",
        "    starts = np.cumsum(n_estimators_per_job)\n",
        "\n",
        "    return n_jobs, n_estimators_per_job.tolist(), [0] + starts.tolist()\n",
        "\n",
        "\n",
        "class RandomScmClassifier(BaseEnsemble, ClassifierMixin):\n",
        "    \"\"\"A Bagging classifier for SetCoveringMachineClassifier()\n",
        "    The base estimators are built on subsets of both samples\n",
        "    and features.\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_estimators : int, default=10\n",
        "        The number of base estimators in the ensemble.\n",
        "    max_samples : int or float, default=1.0\n",
        "        The number of samples to draw from X to train each base estimator with\n",
        "        replacement.\n",
        "        - If int, then draw `max_samples` samples.\n",
        "        - If float, then draw `max_samples * X.shape[0]` samples.\n",
        "    max_features : int or float, default=1.0\n",
        "        The number of features to draw from X to train each base estimator (\n",
        "        without replacement.\n",
        "        - If int, then draw `max_features` features.\n",
        "        - If float, then draw `max_features * X.shape[1]` features.\n",
        "    max_rules : int\n",
        "        maximal number of rules for the scm estimators\n",
        "    p_options : list of float with len =< n_estimators, default=[1.0]\n",
        "        The estimators will be fitted with values of p found in p_options\n",
        "        let k be k = n_estimators/len(p_options),\n",
        "        the k first estimators will have p=p_options[0],\n",
        "        the next k estimators will have p=p_options[1] and so on...\n",
        "    model_type : string, default='conjunction'\n",
        "        type of estimators to build\n",
        "        accepted values : 'conjunction', 'disjunction'\n",
        "    n_jobs : int, default=None\n",
        "        The number of jobs to run in parallel for both fit and\n",
        "        predict. 'None' means 1. '-1' means using all\n",
        "        processors.\n",
        "    random_state : int or RandomState, default=None\n",
        "        Controls the random resampling of the original dataset\n",
        "        (sample wise and feature wise).\n",
        "        If the base estimator accepts a `random_state` attribute, a different\n",
        "        seed is generated for each instance in the ensemble.\n",
        "        Pass an int for reproducible output across multiple function calls.\n",
        "        See :term:`Glossary <random_state>`.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    n_features_ : int\n",
        "        The number of features when :meth:`fit` is performed.\n",
        "    estimators_ : list of estimators\n",
        "        The collection of fitted base estimators.\n",
        "    estim_features : list of arrays\n",
        "        The subset of drawn features for each base estimator.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> random_scm = RandomScmClassifier(p_options=[2, 4], max_samples=0.5, max_features = 0.7)\n",
        "    >>> random_scm.fit(X_train, y_train)\n",
        "    >>> hyperparams = random_scm.get_hyperparams()\n",
        "    >>> importances = random_scm.features_importance()\n",
        "    >>> disagree = random_scm.classifiers_disagreement(X)\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
        "           databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
        "    .. [2] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
        "           Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 n_estimators=100,\n",
        "                 max_samples=0.5,\n",
        "                 max_features=0.5,\n",
        "                 max_rules=10,\n",
        "                 p_options=[1.0],\n",
        "                 model_type=\"conjunction\",\n",
        "                 n_jobs=None,\n",
        "                 min_cq_combination=False,\n",
        "                 min_cq_mu = 10e-3,\n",
        "                 random_state=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_samples = max_samples\n",
        "        self.max_features = max_features\n",
        "        self.max_rules = max_rules\n",
        "        self.p_options = p_options\n",
        "        self.model_type = model_type\n",
        "        self.n_jobs = n_jobs\n",
        "        self.min_cq_combination = min_cq_combination\n",
        "        self.min_cq_mu = min_cq_mu\n",
        "        self.random_state = random_state\n",
        "        self.labels_to_binary = {}\n",
        "        self.binary_to_labels = {}\n",
        "\n",
        "    def p_for_estimators(self):\n",
        "        \"\"\"Return the value of p for each estimator to fit.\"\"\"\n",
        "        options_len = len(self.p_options) # number of options\n",
        "        estims_with_same_p = self.n_estimators // options_len # nb of estimators to fit with the same p\n",
        "        p_of_estims = []\n",
        "        if options_len > 1:\n",
        "            for k in range(options_len - 1):\n",
        "                opt = self.p_options[k] # an option\n",
        "                p_of_estims = p_of_estims + ([opt] * estims_with_same_p) # estims_with_same_p estimators with p=opt\n",
        "        p_of_estims = p_of_estims + ([self.p_options[-1]] * (self.n_estimators - len(p_of_estims)))\n",
        "        return p_of_estims\n",
        "\n",
        "    def get_estimators(self):\n",
        "        \"\"\"Return the list of estimators of the classifier\"\"\"\n",
        "        if hasattr(self, 'estimators'):\n",
        "            return self.estimators\n",
        "        else:\n",
        "            return \"not defined (model not fitted)\"\n",
        "\n",
        "    def get_hyperparams(self):\n",
        "        \"\"\"Return the model hyperparameters\"\"\"\n",
        "        hyperparams = {\n",
        "            'n_estimators' : self.n_estimators, \n",
        "            'max_samples' : self.max_samples, \n",
        "            'max_features' : self.max_features, \n",
        "            'max_rules' : self.max_rules, \n",
        "            'p_options' : self.p_options, \n",
        "            'model_type' : self.model_type, \n",
        "            'random_state' : self.random_state\n",
        "        }\n",
        "        return hyperparams\n",
        "\n",
        "    def labels_conversion(self, labels_list):\n",
        "        \"\"\"\n",
        "        Return the equivalence between labels and binaries\n",
        "        \"\"\"\n",
        "        l = list(set(labels_list))\n",
        "        labels_dict = {c:idx for idx, c in enumerate(l)}\n",
        "        if len(l) < 2:\n",
        "            raise ValueError(\"Only 1 classe given to the model, needs 2\")\n",
        "        elif len(l) > 2:\n",
        "             raise ValueError(\"{} classes were given, multiclass prediction is not implemented\".format(len(l)))\n",
        "        return np.array(l), labels_dict\n",
        "\n",
        "\n",
        "    def fit(self, X, y, get_feature_importances=True, tiebreaker=None):\n",
        "        \"\"\"\n",
        "        Fit the model with the given data\n",
        "        \"\"\"\n",
        "        # Check if 2 classes are inputed and convert labels to binary labels\n",
        "        X, y = check_X_y(X, y)\n",
        "        self.classes_, self.labels_to_binary = self.labels_conversion(y)\n",
        "        self.binary_to_labels = {bin_label:str_label for str_label, bin_label in self.labels_to_binary.items()}\n",
        "        y = np.array([self.labels_to_binary[l] for l in y])\n",
        "\n",
        "        # Save the original number of features\n",
        "        self.n_features = X.shape[1]\n",
        "\n",
        "        self.estimators = []\n",
        "        self.estim_features = []\n",
        "        max_rules = self.max_rules\n",
        "        p_of_estims_ = self.p_for_estimators()\n",
        "        model_type = self.model_type\n",
        "\n",
        "        #seeds for reproductibility\n",
        "        random_state = self.random_state\n",
        "        random_state = check_random_state(random_state)\n",
        "        seeds = random_state.randint(MAX_INT, size=self.n_estimators)\n",
        "        self._seeds = seeds\n",
        "\n",
        "        pop_samples, pop_features = X.shape\n",
        "        max_samples, max_features = self.max_samples, self.max_features\n",
        "\n",
        "        # validate max_samples\n",
        "        if not isinstance(max_samples, numbers.Integral):\n",
        "            max_samples = int(max_samples * pop_samples)\n",
        "        if not (0 < max_samples <= pop_samples):\n",
        "            raise ValueError(\"max_samples must be in (0, n_samples)\")\n",
        "        # store validated integer row sampling values\n",
        "        self._max_samples = max_samples\n",
        "        self._pop_samples = pop_samples\n",
        "\n",
        "        # validate max_features\n",
        "        if isinstance(self.max_features, numbers.Integral):\n",
        "            max_features = self.max_features\n",
        "        elif isinstance(self.max_features, np.float):\n",
        "            max_features = self.max_features * pop_features\n",
        "        else:\n",
        "            raise ValueError(\"max_features must be int or float\")\n",
        "        if not (0 < max_features <= pop_features):\n",
        "            raise ValueError(\"max_features must be in (0, n_features)\")\n",
        "        max_features = max(1, int(max_features))\n",
        "        # store validated integer feature sampling values\n",
        "        self._max_features = max_features\n",
        "        self._pop_features = pop_features\n",
        "\n",
        "        # parallel loop\n",
        "        n_jobs, n_estimators_list, starts = _partition_estimators(self.n_estimators, self.n_jobs)\n",
        "        # building estimators\n",
        "        all_results = Parallel(n_jobs=n_jobs)(delayed(_parallel_build_estimators)(\n",
        "                range(starts[i],starts[i+1]), self, p_of_estims_, seeds, X, y, tiebreaker)\n",
        "            for i in range(n_jobs))\n",
        "\n",
        "        self.estimators += list(itertools.chain.from_iterable(\n",
        "            t[0] for t in all_results))\n",
        "        self.estim_features += list(itertools.chain.from_iterable(\n",
        "            t[1] for t in all_results))\n",
        "\n",
        "        if self.min_cq_combination:\n",
        "            predictions = np.zeros((X.shape[0], self.n_estimators))\n",
        "            for k in range(self.n_estimators):\n",
        "                predictions[:, k] = self.estimators[k].predict(X[:, self.estim_features[k]])\n",
        "\n",
        "            from .min_cq import MinCqLearner\n",
        "            mincq = MinCqLearner(self.min_cq_mu, \"stumps\", n_stumps_per_attribute=1,\n",
        "                                 self_complemented=False)\n",
        "            mincq.fit(predictions, y)\n",
        "            self.min_cq_weights = mincq.majority_vote.weights\n",
        "            self.min_cq_weights /= np.sum(np.abs(self.min_cq_weights))\n",
        "\n",
        "        if get_feature_importances:\n",
        "            importances = self.features_importance()\n",
        "            self.feature_importances_ = np.array([importances['avg'][k]\n",
        "                                                  if k in importances['avg'] else 0\n",
        "                                                  for k in range(self.n_features)])\n",
        "            self.feature_importances_ /= np.sum(self.feature_importances_)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Compute model predictions for data in X\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        predictions : array\n",
        "            predictions[i] is the predicted class for he sample i in X\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, [\"estimators\", \"estim_features\"])\n",
        "        X = check_array(X)\n",
        "        predicted_proba = self.predict_proba(X)\n",
        "        predictions = np.array(np.argmax(predicted_proba, axis=1), dtype=int)\n",
        "        predictions = np.array([self.binary_to_labels[l] for l in predictions])\n",
        "        return predictions\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict class probabilities according to the model estimators\n",
        "\n",
        "        Parameters :\n",
        "        ----------\n",
        "        X : array\n",
        "            a dataset to predict\n",
        "\n",
        "        Returns : \n",
        "        ----------\n",
        "        proba : array\n",
        "            proba[c] contains the estimated probability for each sample of X to belong to class c\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, [\"estimators\", \"estim_features\"])\n",
        "        X = check_array(X)\n",
        "        # parallel loop\n",
        "        n_jobs, n_estimators_list, starts = _partition_estimators(self.n_estimators, self.n_jobs)\n",
        "        results = np.zeros(X.shape[0])\n",
        "        results = Parallel(n_jobs=n_jobs)(delayed(_parallel_predict_proba)(\n",
        "                self, X, range(starts[i], starts[i+1]), results)\n",
        "                for i in range(n_jobs))\n",
        "        if not self.min_cq_combination:\n",
        "            votes = sum(results) / self.n_estimators\n",
        "            proba = np.array([np.array([1 - vote, vote]) for vote in votes])\n",
        "        else:\n",
        "            votes = sum(results)\n",
        "            proba = np.array([np.array([(1 - vote)/2, (1 + vote)/2]) for vote in votes])\n",
        "        return proba\n",
        "\n",
        "    def features_importance(self):\n",
        "        \"\"\"\n",
        "        Compute features importances in estimators rules\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        importance : dict\n",
        "            importances['avg'] : dict (feature id as key, mean importance as value)\n",
        "                The mean importance of each feature over the estimators.\n",
        "            importances['max'] : dict (feature id as key, max importance as value)\n",
        "                The maximal importance of each feature over the estimators.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, [\"estimators\", \"estim_features\"])\n",
        "        importances = {'avg' : {}, 'max' : {}} # average and maximal feature/rule importances\n",
        "        feature_id_occurences = {} # number of occurences of a feature in subsamples\n",
        "        for (estim, features_idx) in zip(self.estimators, self.estim_features):\n",
        "            # increment the total occurences of the feature :\n",
        "            for id_feat in features_idx:\n",
        "                if id_feat in feature_id_occurences:\n",
        "                    feature_id_occurences[id_feat] += 1\n",
        "                else:\n",
        "                    feature_id_occurences[id_feat] = 1\n",
        "            # sum the rules importances :\n",
        "            #rules_importances = estim.get_rules_importances() # activate it when pyscm will implement importance\n",
        "            rules_importances = np.ones(len(estim.model_.rules)) #delete it when pyscm will implement importance\n",
        "            for rule, importance in zip(estim.model_.rules, rules_importances):\n",
        "                global_feat_id = features_idx[rule.feature_idx]\n",
        "                if global_feat_id in importances['avg']:\n",
        "                    importances['avg'][global_feat_id] += importance\n",
        "                    if importance > importances['max'][global_feat_id]:\n",
        "                        importances['max'][global_feat_id] = importance\n",
        "                else:\n",
        "                    importances['avg'][global_feat_id] = importance\n",
        "                    importances['max'][global_feat_id] = importance\n",
        "        importances['avg'] = {k: round(v / feature_id_occurences[k], 3) for k, v in importances['avg'].items()}\n",
        "        return importances\n",
        "\n",
        "    def all_data_tiebreaker(self, model_type, feature_idx, thresholds, rule_type, X, y):\n",
        "        \"\"\"\n",
        "        Choose a rule between rule with equal utility\n",
        "        Select the one that have the best accuracy if applied alone on all the data\n",
        "\n",
        "        Parameters :\n",
        "        ----------\n",
        "        model_type : strint ('conjunction' or 'dijunction')\n",
        "            type of the model\n",
        "        feature_idx, thresholds, rule_type : arrays\n",
        "            description of the rules\n",
        "        X, y : arrays\n",
        "            a dataset used to compare rules\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        ID of the rule to select\n",
        "        \"\"\"\n",
        "        keep_id = 0\n",
        "        keep_id_score = -1\n",
        "        for k in range(len(feature_idx)):\n",
        "            feat_id, threshold, r_type = feature_idx[k], thresholds[k], rule_type[k]\n",
        "            stump = DecisionStump(feature_idx=feat_id, threshold=threshold, kind=r_type)\n",
        "            rule_classif = stump.classify(X).astype('int')\n",
        "            rule_global_score = (rule_classif == y).sum()\n",
        "            if rule_global_score > keep_id_score:\n",
        "                keep_id = k\n",
        "                keep_id_score = rule_global_score\n",
        "        return keep_id\n",
        "\n",
        "    def classifiers_disagreement(self, X):\n",
        "        \"\"\"\n",
        "        Compute disagreement between estimators\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        disagreement : dict \n",
        "            disagreement['global'] : int\n",
        "                global disagreement, computed with the following formula :\n",
        "                average(disagreement(c1, c2))\n",
        "                for (c1, c2) all the pairs of distinct estimators in the model\n",
        "                with \n",
        "                disagreement(c1, c2) = (# of examples where c1 and c2 differs)/(# of examples)\n",
        "            disagreement['estims'] : list\n",
        "                list of disagreements of each estimator with the global model\n",
        "            disagreement['heatmap'] : matrix\n",
        "                heatmap of mutual disagreement for each pair of classifiers\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, [\"estimators\", \"estim_features\"])\n",
        "        disagreement = {'global' : 0, 'estims' : []}\n",
        "        total_comp = 0 # number of comparison of estimators\n",
        "        global_pred = self.predict(X)\n",
        "        heatmap = np.zeros((self.n_estimators, self.n_estimators))\n",
        "        for kA in range(len(self.estimators)):\n",
        "            estimA = self.estimators[kA]\n",
        "            predA = estimA.predict(X[:, self.estim_features[kA]])\n",
        "            disagree = np.not_equal(global_pred, predA).sum()/len(X) # disagreement of estimator A and global model\n",
        "            disagreement['estims'].append(disagree)\n",
        "            for kB in range(len(self.estimators)):\n",
        "                if kB == kA:\n",
        "                    continue # do not compare the estimator with itself\n",
        "                estimB = self.estimators[kB]\n",
        "                predB = estimB.predict(X[:, self.estim_features[kB]])\n",
        "                disagree = np.not_equal(predB, predA).sum()/len(X) # disagreement of estimators A and B\n",
        "                heatmap[kA][kB] = disagree\n",
        "                disagreement['global'] += disagree\n",
        "                total_comp += 1\n",
        "        disagreement['global'] = disagreement['global']/total_comp\n",
        "        disagreement['heatmap'] = heatmap\n",
        "        return disagreement\n",
        "    \n",
        "    def get_estimators_indices(self):\n",
        "        \"\"\"\n",
        "        Get drawn indices along both sample and feature axes\n",
        "        \"\"\"\n",
        "        for seed in self._seeds:\n",
        "            # operations accessing random_state must be performed identically to those in 'fit'\n",
        "            feature_indices = sample_without_replacement(self._pop_features, self._max_features, random_state=seed)\n",
        "            samples_indices = sample_without_replacement(self._pop_samples, self._max_samples, random_state=seed)\n",
        "            yield samples_indices\n",
        "\n",
        "    def score(self, X, y):\n",
        "        check_is_fitted(self, [\"estimators\", \"estim_features\"])\n",
        "        X, y = check_X_y(X, y)\n",
        "        return accuracy_score(y, self.predict(X))"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiePDO-Q1HP-",
        "outputId": "0aa07549-7443-4fa4-c885-7a57aab84138"
      },
      "source": [
        "### Ici on instancie la classe RandomScmClassifier() et on v√©rifie les param√®tres par d√©faut\n",
        "result = RandomScmClassifier()\n",
        "from pprint import pprint\n",
        "# Look at parameters used by our current forest\n",
        "print('Parameters currently in use:\\n')\n",
        "pprint(result.get_params())"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameters currently in use:\n",
            "\n",
            "{'max_features': 0.5,\n",
            " 'max_rules': 10,\n",
            " 'max_samples': 0.5,\n",
            " 'min_cq_combination': False,\n",
            " 'min_cq_mu': 0.01,\n",
            " 'model_type': 'conjunction',\n",
            " 'n_estimators': 100,\n",
            " 'n_jobs': None,\n",
            " 'p_options': [1.0],\n",
            " 'random_state': None}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPwYlBCz22NR"
      },
      "source": [
        "### Ici on divise les donn√©es avec un train/test de 80/20 et on rajoute un seed de 42\n",
        "from sklearn.model_selection import train_test_split \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq6KAnXN3J4D"
      },
      "source": [
        "### Ici on entraine le mod√®le SCM avec les param√®tres par d√©faut:\n",
        "result.fit(X_train, y_train)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBgSqo8d3Wyw",
        "outputId": "74f23c45-48e7-4d29-a48e-96397e72af6b"
      },
      "source": [
        "### Ici on calcule les pr√©dictions avec le X_test puis on calcule la Test Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "predictions = result.predict(X_test)\n",
        "print(\"Test Accuracy:\", metrics.accuracy_score(y_test, predictions))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ba1P99V73wJX",
        "outputId": "b88c8df9-3f8c-455f-e352-aa0ff379063c"
      },
      "source": [
        "### Ici, v√©rification avec f1_score\n",
        "from sklearn.metrics import f1_score\n",
        "f1_score(y_test, predictions)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14285714285714288"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raY15kVxGayQ",
        "outputId": "973daf5d-847b-469e-a55b-2aa672e9c434"
      },
      "source": [
        "### Ici on reentraine le mod√®le avec du Boostrapping 60 fois\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn import metrics\n",
        "#### Bootstrapping ####\n",
        "########################################################\n",
        "# Creating empty list to hold accuracy values\n",
        "AccuracyValues=[]\n",
        "n_times=60\n",
        "## Performing bootstrapping\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split \n",
        "for i in range(n_times):\n",
        "    #Split the data into training and testing set\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    # Changing the seed value for each iteration\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42+i)\n",
        "    result = RandomScmClassifier(max_features = 0.5, max_samples = 0.5, model_type = 'disjunction', n_estimators = 10, p_options = [5.0])\n",
        "    result.fit(X_train, y_train)\n",
        "    ########################################################\n",
        "    prediction = result.predict(X_test)\n",
        "    Accuracy=metrics.accuracy_score(y_test, prediction)\n",
        "    print(Accuracy)\n",
        "    AccuracyValues.append((Accuracy))\n",
        "    #print(Accuracy)\n",
        "    print(AccuracyValues)\n",
        "     #Measuring accuracy on Testing Data\n",
        "#Accuracy=100- (np.mean(np.abs((y_test - prediction) / y_test)) * 100)\n",
        "    \n",
        "    # Storing accuracy values\n",
        "#print(AccuracyValues.append(np.round(Accuracy)))\n",
        "    \n",
        "################################################\n",
        "# Result of all bootstrapping trials\n",
        "#print(AccuracyValues)\n",
        " \n",
        "# Final accuracy\n",
        "print('Final average accuracy',np.mean(AccuracyValues))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5\n",
            "[0.5]\n",
            "0.6\n",
            "[0.5, 0.6]\n",
            "0.45\n",
            "[0.5, 0.6, 0.45]\n",
            "0.7\n",
            "[0.5, 0.6, 0.45, 0.7]\n",
            "0.5\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5]\n",
            "0.6\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6]\n",
            "0.5\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5]\n",
            "0.5\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5]\n",
            "0.55\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55]\n",
            "0.5\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5]\n",
            "0.45\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45]\n",
            "0.55\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55]\n",
            "0.7\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7]\n",
            "0.8\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8]\n",
            "0.45\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45]\n",
            "0.5\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5]\n",
            "0.7\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7]\n",
            "0.55\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55]\n",
            "0.35\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35]\n",
            "0.55\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55]\n",
            "0.6\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6]\n",
            "0.4\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4]\n",
            "0.6\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6]\n",
            "0.6\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6]\n",
            "0.5\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5]\n",
            "0.4\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4]\n",
            "0.5\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5]\n",
            "0.55\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55]\n",
            "0.5\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5]\n",
            "0.6\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6]\n",
            "0.4\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4]\n",
            "0.6\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6]\n",
            "0.35\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35]\n",
            "0.45\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45]\n",
            "0.6\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6]\n",
            "0.45\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45]\n",
            "0.35\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35]\n",
            "0.4\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4]\n",
            "0.5\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5]\n",
            "0.4\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5, 0.4]\n",
            "0.6\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5, 0.4, 0.6]\n",
            "0.5\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5, 0.4, 0.6, 0.5]\n",
            "0.4\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5, 0.4, 0.6, 0.5, 0.4]\n",
            "0.35\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5, 0.4, 0.6, 0.5, 0.4, 0.35]\n",
            "0.55\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5, 0.4, 0.6, 0.5, 0.4, 0.35, 0.55]\n",
            "0.4\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5, 0.4, 0.6, 0.5, 0.4, 0.35, 0.55, 0.4]\n",
            "0.6\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5, 0.4, 0.6, 0.5, 0.4, 0.35, 0.55, 0.4, 0.6]\n",
            "0.6\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5, 0.4, 0.6, 0.5, 0.4, 0.35, 0.55, 0.4, 0.6, 0.6]\n",
            "0.55\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5, 0.4, 0.6, 0.5, 0.4, 0.35, 0.55, 0.4, 0.6, 0.6, 0.55]\n",
            "0.8\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5, 0.4, 0.6, 0.5, 0.4, 0.35, 0.55, 0.4, 0.6, 0.6, 0.55, 0.8]\n",
            "0.7\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5, 0.4, 0.6, 0.5, 0.4, 0.35, 0.55, 0.4, 0.6, 0.6, 0.55, 0.8, 0.7]\n",
            "0.5\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5, 0.4, 0.6, 0.5, 0.4, 0.35, 0.55, 0.4, 0.6, 0.6, 0.55, 0.8, 0.7, 0.5]\n",
            "0.6\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5, 0.4, 0.6, 0.5, 0.4, 0.35, 0.55, 0.4, 0.6, 0.6, 0.55, 0.8, 0.7, 0.5, 0.6]\n",
            "0.45\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5, 0.4, 0.6, 0.5, 0.4, 0.35, 0.55, 0.4, 0.6, 0.6, 0.55, 0.8, 0.7, 0.5, 0.6, 0.45]\n",
            "0.5\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5, 0.4, 0.6, 0.5, 0.4, 0.35, 0.55, 0.4, 0.6, 0.6, 0.55, 0.8, 0.7, 0.5, 0.6, 0.45, 0.5]\n",
            "0.7\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5, 0.4, 0.6, 0.5, 0.4, 0.35, 0.55, 0.4, 0.6, 0.6, 0.55, 0.8, 0.7, 0.5, 0.6, 0.45, 0.5, 0.7]\n",
            "0.5\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5, 0.4, 0.6, 0.5, 0.4, 0.35, 0.55, 0.4, 0.6, 0.6, 0.55, 0.8, 0.7, 0.5, 0.6, 0.45, 0.5, 0.7, 0.5]\n",
            "0.5\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5, 0.4, 0.6, 0.5, 0.4, 0.35, 0.55, 0.4, 0.6, 0.6, 0.55, 0.8, 0.7, 0.5, 0.6, 0.45, 0.5, 0.7, 0.5, 0.5]\n",
            "0.45\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5, 0.4, 0.6, 0.5, 0.4, 0.35, 0.55, 0.4, 0.6, 0.6, 0.55, 0.8, 0.7, 0.5, 0.6, 0.45, 0.5, 0.7, 0.5, 0.5, 0.45]\n",
            "0.65\n",
            "[0.5, 0.6, 0.45, 0.7, 0.5, 0.6, 0.5, 0.5, 0.55, 0.5, 0.45, 0.55, 0.7, 0.8, 0.45, 0.5, 0.7, 0.55, 0.35, 0.55, 0.6, 0.4, 0.6, 0.6, 0.5, 0.4, 0.5, 0.55, 0.5, 0.6, 0.4, 0.6, 0.35, 0.45, 0.6, 0.45, 0.35, 0.4, 0.5, 0.4, 0.6, 0.5, 0.4, 0.35, 0.55, 0.4, 0.6, 0.6, 0.55, 0.8, 0.7, 0.5, 0.6, 0.45, 0.5, 0.7, 0.5, 0.5, 0.45, 0.65]\n",
            "Final average accuracy 0.5275000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JW35_EWE4Fgv",
        "outputId": "e7a9e950-1cb7-46af-93b9-f33f7241ec9c"
      },
      "source": [
        "### Ici on commence la Cross Validation pour trouver la meilleure combinaison de param√®tres\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# The number of base estimators in the ensemble.\n",
        "n_estimators = [1, 10, 100, 1000] \n",
        "# Number of maximum features The number of features to draw from X to train each base estimator (without replacement.)\n",
        "max_features = [0.25, 0.5, 0.75, 1.0] \n",
        "# Maximum samples\n",
        "max_samples = [0.25, 0.5, 0.75, 1.0]\n",
        "# p_options: The estimators will be fitted with values of p found in p_options \n",
        "p_options = [[0.1], [0.5], [1.0], [5.0], [10.0]]\n",
        "# model_type: type of estimators to build\n",
        "model_type = ['conjunction', 'disjunction']\n",
        "# bootstrap\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_samples': max_samples,\n",
        "               'p_options': p_options,\n",
        "               'model_type': model_type}\n",
        "pprint(random_grid)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'max_features': [0.25, 0.5, 0.75, 1.0],\n",
            " 'max_samples': [0.25, 0.5, 0.75, 1.0],\n",
            " 'model_type': ['conjunction', 'disjunction'],\n",
            " 'n_estimators': [1, 10, 100, 1000],\n",
            " 'p_options': [[0.1], [0.5], [1.0], [5.0], [10.0]]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5004T1qmFLFz",
        "outputId": "55a1b84e-987b-4576-9c06-fa74c5d7b879"
      },
      "source": [
        "### Ici on entra√Æne les diff√©rentes combinaisons de param√®tres\n",
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "#rf = RandomForestClassifier()\n",
        "CVresult = RandomScmClassifier()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "CVresult = RandomizedSearchCV(estimator = CVresult, param_distributions = random_grid, n_iter = 200, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "CVresult.fit(X_train, y_train)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   22.8s\n",
            "[Parallel(n_jobs=-1)]: Done 164 tasks      | elapsed:  2.0min\n",
            "[Parallel(n_jobs=-1)]: Done 375 tasks      | elapsed:  3.8min\n",
            "[Parallel(n_jobs=-1)]: Done 734 tasks      | elapsed:  6.8min\n",
            "[Parallel(n_jobs=-1)]: Done 997 out of 1000 | elapsed:  8.2min remaining:    1.5s\n",
            "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  8.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=5, error_score=nan,\n",
              "                   estimator=RandomScmClassifier(max_features=0.5, max_rules=10,\n",
              "                                                 max_samples=0.5,\n",
              "                                                 min_cq_combination=False,\n",
              "                                                 min_cq_mu=0.01,\n",
              "                                                 model_type='conjunction',\n",
              "                                                 n_estimators=100, n_jobs=None,\n",
              "                                                 p_options=[1.0],\n",
              "                                                 random_state=None),\n",
              "                   iid='deprecated', n_iter=200, n_jobs=-1,\n",
              "                   param_distributions={'max_features': [0.25, 0.5, 0.75, 1.0],\n",
              "                                        'max_samples': [0.25, 0.5, 0.75, 1.0],\n",
              "                                        'model_type': ['conjunction',\n",
              "                                                       'disjunction'],\n",
              "                                        'n_estimators': [1, 10, 100, 1000],\n",
              "                                        'p_options': [[0.1], [0.5], [1.0],\n",
              "                                                      [5.0], [10.0]]},\n",
              "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMlSWrYQM42n",
        "outputId": "039d7486-4877-456b-eb45-90382d359294"
      },
      "source": [
        "CVresult.best_params_   ### Meilleurs param√®tres obtenus en Cross-Validation"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_features': 1.0,\n",
              " 'max_samples': 0.5,\n",
              " 'model_type': 'disjunction',\n",
              " 'n_estimators': 100,\n",
              " 'p_options': [5.0]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhjS_rhYKPhj"
      },
      "source": [
        "### Nouvelle instantiation du RandomSCM avec les param√®tres trouv√©s en Cross-Validation\n",
        "CVresult = RandomScmClassifier(max_features = 1.0, max_samples = 0.5, model_type = 'disjunction', n_estimators = 100, p_options = [5.0])"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3hK75P4Lkpv"
      },
      "source": [
        "### entra√Ænement du mod√®le avec les param√®tres trouv√©s en Cross-Validation\n",
        "from sklearn.model_selection import train_test_split \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "CVresult.fit(X_train, y_train)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hoakwdWLxQ1",
        "outputId": "c34b20b4-f83a-485e-c2c6-09ad58b724ab"
      },
      "source": [
        "### Calcul de y_test et de l'accuracy \n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "predictions = CVresult.predict(X_test)\n",
        "print(\"Test Accuracy:\", metrics.accuracy_score(y_test, predictions))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZeXvOioL8rb",
        "outputId": "5ad3891a-0c84-4293-f3e4-b22ba463bb24"
      },
      "source": [
        "### V√©rification avec f1_score\n",
        "from sklearn.metrics import f1_score\n",
        "f1_score(y_test, predictions)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6363636363636364"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hamyJHxtMDU1",
        "outputId": "2d737b72-309d-4473-ed00-90ce61f64978"
      },
      "source": [
        "### Boostraping 60 fois avec Param√®tres obtenus en Cross-Validation\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn import metrics\n",
        "#### Bootstrapping ####\n",
        "########################################################\n",
        "# Creating empty list to hold accuracy values\n",
        "AccuracyValues=[]\n",
        "n_times=60\n",
        "## Performing bootstrapping\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split \n",
        "for i in range(n_times):\n",
        "    #Split the data into training and testing set\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    # Changing the seed value for each iteration\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42+i)\n",
        "    CVresult = RandomScmClassifier(max_features = 1.0, max_samples = 0.5, model_type = 'disjunction', n_estimators = 100, p_options = [5.0])\n",
        "    CVresult.fit(X_train, y_train)\n",
        "    ########################################################\n",
        "    prediction = CVresult.predict(X_test)\n",
        "    Accuracy=metrics.accuracy_score(y_test, prediction)\n",
        "    print(Accuracy)\n",
        "    AccuracyValues.append((Accuracy))\n",
        "    #print(Accuracy)\n",
        "    print(AccuracyValues)\n",
        "    \n",
        "    #Creating the model on Training Data\n",
        "    #DTree=RegModel.fit(X_train,y_train)\n",
        "    #prediction=DTree.predict(X_test)\n",
        " \n",
        "    #Measuring accuracy on Testing Data\n",
        "#Accuracy=100- (np.mean(np.abs((y_test - prediction) / y_test)) * 100)\n",
        "    \n",
        "    # Storing accuracy values\n",
        "#AccuracyValues.append(np.round(Accuracy))\n",
        "    \n",
        "################################################\n",
        "# Result of all bootstrapping trials\n",
        "print(AccuracyValues)\n",
        " \n",
        "# Final accuracy\n",
        "print('Final average accuracy',np.mean(AccuracyValues))\n",
        "#print(\"Test Accuracy:\", metrics.accuracy_score(y_test, y_final ))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6\n",
            "[0.6]\n",
            "0.6\n",
            "[0.6, 0.6]\n",
            "0.4\n",
            "[0.6, 0.6, 0.4]\n",
            "0.7\n",
            "[0.6, 0.6, 0.4, 0.7]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6]\n",
            "0.7\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6]\n",
            "0.55\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55]\n",
            "0.35\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5]\n",
            "0.7\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6]\n",
            "0.75\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75]\n",
            "0.65\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65]\n",
            "0.65\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5]\n",
            "0.45\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45]\n",
            "0.55\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55]\n",
            "0.65\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6]\n",
            "0.7\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5]\n",
            "0.45\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45]\n",
            "0.7\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7]\n",
            "0.4\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4]\n",
            "0.45\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45]\n",
            "0.55\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55]\n",
            "0.45\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45]\n",
            "0.55\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55]\n",
            "0.65\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6]\n",
            "0.55\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5]\n",
            "0.55\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5]\n",
            "0.4\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4]\n",
            "0.65\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6]\n",
            "0.55\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55]\n",
            "0.55\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55]\n",
            "0.45\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45]\n",
            "0.4\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4]\n",
            "0.7\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7, 0.6]\n",
            "0.65\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7, 0.6, 0.65]\n",
            "0.45\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7, 0.6, 0.65, 0.45]\n",
            "0.45\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7, 0.6, 0.65, 0.45, 0.45]\n",
            "0.7\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7, 0.6, 0.65, 0.45, 0.45, 0.7]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7, 0.6, 0.65, 0.45, 0.45, 0.7, 0.6]\n",
            "0.45\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7, 0.6, 0.65, 0.45, 0.45, 0.7, 0.6, 0.45]\n",
            "0.55\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7, 0.6, 0.65, 0.45, 0.45, 0.7, 0.6, 0.45, 0.55]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7, 0.6, 0.65, 0.45, 0.45, 0.7, 0.6, 0.45, 0.55, 0.5]\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7, 0.6, 0.65, 0.45, 0.45, 0.7, 0.6, 0.45, 0.55, 0.5]\n",
            "Final average accuracy 0.5558333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}