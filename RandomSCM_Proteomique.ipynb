{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RandomSCM_MÃ©tabolomique",
      "provenance": [],
      "authorship_tag": "ABX9TyMoiv8/y+wSW2bpw0qnS9rC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samibahig/RecoverProject/blob/main/RandomSCM_Proteomique.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps9eLuQTBnv8",
        "outputId": "52d86d0e-255c-47a1-b3b8-1f19669fb05c"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle as pkl\n",
        "#print('\\nMETADATA :')\n",
        "data_path = ''\n",
        "metadata_filename = '/content/metadata.csv'\n",
        "meta_df = pd.read_csv(metadata_filename)\n",
        "print('-----------')\n",
        "print(meta_df)\n",
        "print('-----------')\n",
        "#print(meta_df.columns)\n",
        "meta_df.columns = ['#', 'plate', '-', 'symptoms'] + list(meta_df)[4:]\n",
        "#print(meta_df.columns)\n",
        "#print('available metadata :', list(meta_df))\n",
        "meta_idx = meta_df['ID'].to_list()\n",
        "meta_label = meta_df['symptoms'].to_list()\n",
        "#print('------------------')\n",
        "#print(list(zip(meta_idx, meta_label)))\n",
        "#print('------------------')\n",
        "meta_id_label_dict = {str(k): 1 if v=='S' else 0 for k, v in zip(meta_idx, meta_label)}\n",
        "\n",
        "#DF1 : proteomics\n",
        "#print('\\nPROEOMICS DATA :')\n",
        "proteomics_data_filename = '/content/proteomics.csv'\n",
        "\n",
        "dim_df = pd.read_csv(proteomics_data_filename, nrows=1)\n",
        "#print('--------')\n",
        "#print(dim_df)\n",
        "#print('--------')\n",
        "dim = len(list(dim_df))\n",
        "#print(dim)\n",
        "#print('------------')\n",
        "print('# of columns in source csv file :', dim)\n",
        "all_cols = [i for i in range(dim)]\n",
        "\n",
        "print('--------')\n",
        "feat_cols = all_cols[1:-4]\n",
        "print(feat_cols)\n",
        "print('--------')\n",
        "samplesidx_col = [0]\n",
        "\n",
        "feat_df = pd.read_csv(proteomics_data_filename, skiprows=4, nrows=1, dtype=str, usecols=feat_cols)\n",
        "features = list(feat_df)\n",
        "print('# of features : ', len(features))\n",
        "print('first feature :', features[0])\n",
        "print('last feature :', features[-1])\n",
        "\n",
        "idx_df = pd.read_csv(proteomics_data_filename, skiprows=6, index_col=0, skipfooter=4, usecols=[0], engine='python')\n",
        "idx = list(idx_df.index.values)\n",
        "print('# of idx : ', len(idx))\n",
        "print('first id :', idx[0])\n",
        "print('last id :', idx[-1])\n",
        "\n",
        "df1 = pd.read_csv(proteomics_data_filename, skiprows=6, dtype=np.float32, skipfooter=4, usecols=feat_cols, engine='python')\n",
        "assert df1.shape[0] == len(idx)\n",
        "assert df1.shape[1] == len(features)\n",
        "\n",
        "df1['idx'] = idx\n",
        "df1.set_index('idx', inplace=True)\n",
        "df1.columns = features\n",
        "print('# of Nan values :', df1.isna().sum().sum())\n",
        "\n",
        "#clean data of samples that are not in metadata :\n",
        "idx = df1.index.values\n",
        "y = []\n",
        "for k in range(len(idx)):\n",
        "    id = idx[k]\n",
        "    if id in meta_id_label_dict:\n",
        "        y.append(meta_id_label_dict[id])\n",
        "    else:\n",
        "        # we will not put this sample in the dataset\n",
        "        #print('sample to remove because of unknown label:', k, id)\n",
        "        y.append('to_remove')\n",
        "df1['label'] = y\n",
        "df1 = df1[df1.label != 'to_remove']\n",
        "\n",
        "#create X and y matrices for ML :\n",
        "y = list(df1['label'])\n",
        "del df1['label']\n",
        "print('---------')\n",
        "print(df1)\n",
        "print('---------')\n",
        "X = df1.to_numpy()\n",
        "print('proteomics data :')\n",
        "print('# of samples : ', df1.shape[0])\n",
        "print('# of features : ', df1.shape[1])\n",
        "print('labels:', list(dict.fromkeys(y)))\n",
        "\n",
        "\n",
        "## save X and y in pickles if you want :\n",
        "##data_name = 'recover_multiomics_'\n",
        "##feat_dict = {k: str(v) for k, v in zip(range(len(list(df))), list(df))}\n",
        "##with open(data_path + data_name + 'feat_dict', 'wb') as fo:\n",
        "##    pkl.dump(feat_dict, fo)\n",
        "##with open(data_path + data_name + 'X', 'wb') as fo:df\n",
        "##            pkl.dump(X, fo)\n",
        "##with open(data_path + data_name + 'y', 'wb') as fo:\n",
        "##            pkl.dump(y, fo)\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------\n",
            "    Unnamed: 0 Unnamed: 1  ... addition                                specify.1\n",
            "0            1    Plate 1  ...      NaN                                      NaN\n",
            "1            2    Plate 1  ...      5.0                                   Stress\n",
            "2            3    Plate 1  ...      6.0  back pain is worse, shortness of breath\n",
            "3            4    Plate 1  ...      2.0  Impression of a foreingh body in throat\n",
            "4            5    Plate 1  ...      2.0                                      NaN\n",
            "..         ...        ...  ...      ...                                      ...\n",
            "95         100    Plate 2  ...      NaN                                      NaN\n",
            "96         101    Plate 2  ...      NaN                                      NaN\n",
            "97         102    Plate 2  ...      NaN                                      NaN\n",
            "98         103    Plate 2  ...      NaN                                      NaN\n",
            "99         104    Plate 2  ...      NaN                                      NaN\n",
            "\n",
            "[100 rows x 46 columns]\n",
            "-----------\n",
            "# of columns in source csv file : 189\n",
            "--------\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184]\n",
            "--------\n",
            "# of features :  184\n",
            "first feature : Q96SB3\n",
            "last feature : P09603\n",
            "# of idx :  104\n",
            "first id : 5-139\n",
            "last id : 3-043\n",
            "# of Nan values : 0\n",
            "---------\n",
            "        Q96SB3   P16278   O75475   Q05516  ...  P05113.1   P00813   P01374    P09603\n",
            "idx                                        ...                                      \n",
            "5-139  1.61741  1.31276  3.20077  1.28369  ...   0.12752  5.46542  5.97368  10.40443\n",
            "1-039  2.79530  2.12398  2.90156  1.55239  ...   0.88631  5.24029  5.07646  10.14180\n",
            "1-062  1.69202  2.07015  2.07562  0.85847  ...   0.18121  5.34243  4.89149  10.02279\n",
            "1-040  1.62496  1.67346  2.30191  1.09831  ...   2.11383  5.54121  5.57492  10.13417\n",
            "2-044  1.69246  1.94635  1.87725  0.84233  ...   0.60991  4.98001  4.80933  10.04586\n",
            "...        ...      ...      ...      ...  ...       ...      ...      ...       ...\n",
            "1-073  1.40763  2.32194  2.53494  0.55014  ...   0.31641  5.31388  4.78352  10.50358\n",
            "2-108  1.73766  1.44918  2.23165  0.44999  ...   0.37494  5.44716  5.22955  10.13842\n",
            "5-114  2.94638  1.45486  3.31485  0.93572  ...   2.98474  5.34230  5.30634  10.16184\n",
            "5-075  2.09489  2.70359  2.17787  1.16964  ...   0.76538  5.14975  5.37112  10.40146\n",
            "3-043  2.23329  2.41564  3.45536  0.91918  ...   0.87126  5.34694  5.08457  10.35240\n",
            "\n",
            "[100 rows x 184 columns]\n",
            "---------\n",
            "proteomics data :\n",
            "# of samples :  100\n",
            "# of features :  184\n",
            "labels: [1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh-qYeoYwOQq"
      },
      "source": [
        "from sklearn.base import ClassifierMixin\n",
        "from sklearn.ensemble import BaseEnsemble\n",
        "from pyscm import SetCoveringMachineClassifier\n",
        "from pyscm.rules import DecisionStump\n",
        "\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted, check_random_state\n",
        "from sklearn.utils.random import sample_without_replacement\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numbers\n",
        "import itertools\n",
        "import numpy as np\n",
        "from warnings import warn\n",
        "from joblib import effective_n_jobs, Parallel, delayed\n",
        "\n",
        "MAX_INT = np.iinfo(np.int32).max\n",
        "def _parallel_build_estimators(idx, ensemble, p_of_estims, seeds, X, y, tiebreaker):\n",
        "    \"\"\"\n",
        "    Fit SCM estimators on subsamples of the training data\n",
        "    \"\"\"\n",
        "    estimators = []\n",
        "    estim_features = []\n",
        "    for k in idx:\n",
        "        p_param = p_of_estims[k] # p param for the classifier to fit\n",
        "        random_state = seeds[k]\n",
        "        estim = SetCoveringMachineClassifier(p=p_param, max_rules=ensemble.max_rules, model_type=ensemble.model_type, random_state=random_state)\n",
        "        feature_indices = sample_without_replacement(ensemble._pop_features, ensemble._max_features, random_state=random_state)\n",
        "        samples_indices = sample_without_replacement(ensemble._pop_samples, ensemble._max_samples, random_state=random_state)\n",
        "        Xk = (X[samples_indices])[:, feature_indices]\n",
        "        yk = y[samples_indices]\n",
        "        if len(list(set(yk))) < 2:\n",
        "            raise ValueError(\"One of the subsamples contains elements from only 1 class, try increase max_samples value\")\n",
        "        if tiebreaker is None:\n",
        "            estim.fit(Xk, yk)\n",
        "        else:\n",
        "            estim.fit(Xk, yk, tiebreaker)\n",
        "        estim_features.append(feature_indices)\n",
        "        estimators.append(estim)\n",
        "    return estimators, estim_features\n",
        "\n",
        "\n",
        "def _parallel_predict_proba(ensemble, X, idx, results):\n",
        "    \"\"\"\n",
        "    Compute predictions of SCM estimators\n",
        "    \"\"\"\n",
        "    for k in idx:\n",
        "        res = ensemble.estimators[k].predict(X[:, ensemble.estim_features[k]])\n",
        "        if ensemble.min_cq_combination:\n",
        "            res[res==0] = -1\n",
        "            results = results + ensemble.min_cq_weights[k]*res\n",
        "        else:\n",
        "            results = results + res\n",
        "    return results\n",
        "\n",
        "\n",
        "def _partition_estimators(n_estimators, n_jobs):\n",
        "    \"\"\"Private function used to partition estimators between jobs.\"\"\"\n",
        "    # Compute the number of jobs\n",
        "    n_jobs = min(effective_n_jobs(n_jobs), n_estimators)\n",
        "\n",
        "    # Partition estimators between jobs\n",
        "    n_estimators_per_job = np.full(n_jobs, n_estimators // n_jobs,\n",
        "                                   dtype=np.int)\n",
        "    n_estimators_per_job[:n_estimators % n_jobs] += 1\n",
        "    starts = np.cumsum(n_estimators_per_job)\n",
        "\n",
        "    return n_jobs, n_estimators_per_job.tolist(), [0] + starts.tolist()\n",
        "\n",
        "\n",
        "class RandomScmClassifier(BaseEnsemble, ClassifierMixin):\n",
        "    \"\"\"A Bagging classifier for SetCoveringMachineClassifier()\n",
        "    The base estimators are built on subsets of both samples\n",
        "    and features.\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_estimators : int, default=10\n",
        "        The number of base estimators in the ensemble.\n",
        "    max_samples : int or float, default=1.0\n",
        "        The number of samples to draw from X to train each base estimator with\n",
        "        replacement.\n",
        "        - If int, then draw `max_samples` samples.\n",
        "        - If float, then draw `max_samples * X.shape[0]` samples.\n",
        "    max_features : int or float, default=1.0\n",
        "        The number of features to draw from X to train each base estimator (\n",
        "        without replacement.\n",
        "        - If int, then draw `max_features` features.\n",
        "        - If float, then draw `max_features * X.shape[1]` features.\n",
        "    max_rules : int\n",
        "        maximal number of rules for the scm estimators\n",
        "    p_options : list of float with len =< n_estimators, default=[1.0]\n",
        "        The estimators will be fitted with values of p found in p_options\n",
        "        let k be k = n_estimators/len(p_options),\n",
        "        the k first estimators will have p=p_options[0],\n",
        "        the next k estimators will have p=p_options[1] and so on...\n",
        "    model_type : string, default='conjunction'\n",
        "        type of estimators to build\n",
        "        accepted values : 'conjunction', 'disjunction'\n",
        "    n_jobs : int, default=None\n",
        "        The number of jobs to run in parallel for both fit and\n",
        "        predict. 'None' means 1. '-1' means using all\n",
        "        processors.\n",
        "    random_state : int or RandomState, default=None\n",
        "        Controls the random resampling of the original dataset\n",
        "        (sample wise and feature wise).\n",
        "        If the base estimator accepts a `random_state` attribute, a different\n",
        "        seed is generated for each instance in the ensemble.\n",
        "        Pass an int for reproducible output across multiple function calls.\n",
        "        See :term:`Glossary <random_state>`.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    n_features_ : int\n",
        "        The number of features when :meth:`fit` is performed.\n",
        "    estimators_ : list of estimators\n",
        "        The collection of fitted base estimators.\n",
        "    estim_features : list of arrays\n",
        "        The subset of drawn features for each base estimator.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> random_scm = RandomScmClassifier(p_options=[2, 4], max_samples=0.5, max_features = 0.7)\n",
        "    >>> random_scm.fit(X_train, y_train)\n",
        "    >>> hyperparams = random_scm.get_hyperparams()\n",
        "    >>> importances = random_scm.features_importance()\n",
        "    >>> disagree = random_scm.classifiers_disagreement(X)\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
        "           databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
        "    .. [2] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
        "           Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 n_estimators=100,\n",
        "                 max_samples=0.5,\n",
        "                 max_features=0.5,\n",
        "                 max_rules=10,\n",
        "                 p_options=[1.0],\n",
        "                 model_type=\"conjunction\",\n",
        "                 n_jobs=None,\n",
        "                 min_cq_combination=False,\n",
        "                 min_cq_mu = 10e-3,\n",
        "                 random_state=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_samples = max_samples\n",
        "        self.max_features = max_features\n",
        "        self.max_rules = max_rules\n",
        "        self.p_options = p_options\n",
        "        self.model_type = model_type\n",
        "        self.n_jobs = n_jobs\n",
        "        self.min_cq_combination = min_cq_combination\n",
        "        self.min_cq_mu = min_cq_mu\n",
        "        self.random_state = random_state\n",
        "        self.labels_to_binary = {}\n",
        "        self.binary_to_labels = {}\n",
        "\n",
        "    def p_for_estimators(self):\n",
        "        \"\"\"Return the value of p for each estimator to fit.\"\"\"\n",
        "        options_len = len(self.p_options) # number of options\n",
        "        estims_with_same_p = self.n_estimators // options_len # nb of estimators to fit with the same p\n",
        "        p_of_estims = []\n",
        "        if options_len > 1:\n",
        "            for k in range(options_len - 1):\n",
        "                opt = self.p_options[k] # an option\n",
        "                p_of_estims = p_of_estims + ([opt] * estims_with_same_p) # estims_with_same_p estimators with p=opt\n",
        "        p_of_estims = p_of_estims + ([self.p_options[-1]] * (self.n_estimators - len(p_of_estims)))\n",
        "        return p_of_estims\n",
        "\n",
        "    def get_estimators(self):\n",
        "        \"\"\"Return the list of estimators of the classifier\"\"\"\n",
        "        if hasattr(self, 'estimators'):\n",
        "            return self.estimators\n",
        "        else:\n",
        "            return \"not defined (model not fitted)\"\n",
        "\n",
        "    def get_hyperparams(self):\n",
        "        \"\"\"Return the model hyperparameters\"\"\"\n",
        "        hyperparams = {\n",
        "            'n_estimators' : self.n_estimators, \n",
        "            'max_samples' : self.max_samples, \n",
        "            'max_features' : self.max_features, \n",
        "            'max_rules' : self.max_rules, \n",
        "            'p_options' : self.p_options, \n",
        "            'model_type' : self.model_type, \n",
        "            'random_state' : self.random_state\n",
        "        }\n",
        "        return hyperparams\n",
        "\n",
        "    def labels_conversion(self, labels_list):\n",
        "        \"\"\"\n",
        "        Return the equivalence between labels and binaries\n",
        "        \"\"\"\n",
        "        l = list(set(labels_list))\n",
        "        labels_dict = {c:idx for idx, c in enumerate(l)}\n",
        "        if len(l) < 2:\n",
        "            raise ValueError(\"Only 1 classe given to the model, needs 2\")\n",
        "        elif len(l) > 2:\n",
        "             raise ValueError(\"{} classes were given, multiclass prediction is not implemented\".format(len(l)))\n",
        "        return np.array(l), labels_dict\n",
        "\n",
        "\n",
        "    def fit(self, X, y, get_feature_importances=True, tiebreaker=None):\n",
        "        \"\"\"\n",
        "        Fit the model with the given data\n",
        "        \"\"\"\n",
        "        # Check if 2 classes are inputed and convert labels to binary labels\n",
        "        X, y = check_X_y(X, y)\n",
        "        self.classes_, self.labels_to_binary = self.labels_conversion(y)\n",
        "        self.binary_to_labels = {bin_label:str_label for str_label, bin_label in self.labels_to_binary.items()}\n",
        "        y = np.array([self.labels_to_binary[l] for l in y])\n",
        "\n",
        "        # Save the original number of features\n",
        "        self.n_features = X.shape[1]\n",
        "\n",
        "        self.estimators = []\n",
        "        self.estim_features = []\n",
        "        max_rules = self.max_rules\n",
        "        p_of_estims_ = self.p_for_estimators()\n",
        "        model_type = self.model_type\n",
        "\n",
        "        #seeds for reproductibility\n",
        "        random_state = self.random_state\n",
        "        random_state = check_random_state(random_state)\n",
        "        seeds = random_state.randint(MAX_INT, size=self.n_estimators)\n",
        "        self._seeds = seeds\n",
        "\n",
        "        pop_samples, pop_features = X.shape\n",
        "        max_samples, max_features = self.max_samples, self.max_features\n",
        "\n",
        "        # validate max_samples\n",
        "        if not isinstance(max_samples, numbers.Integral):\n",
        "            max_samples = int(max_samples * pop_samples)\n",
        "        if not (0 < max_samples <= pop_samples):\n",
        "            raise ValueError(\"max_samples must be in (0, n_samples)\")\n",
        "        # store validated integer row sampling values\n",
        "        self._max_samples = max_samples\n",
        "        self._pop_samples = pop_samples\n",
        "\n",
        "        # validate max_features\n",
        "        if isinstance(self.max_features, numbers.Integral):\n",
        "            max_features = self.max_features\n",
        "        elif isinstance(self.max_features, np.float):\n",
        "            max_features = self.max_features * pop_features\n",
        "        else:\n",
        "            raise ValueError(\"max_features must be int or float\")\n",
        "        if not (0 < max_features <= pop_features):\n",
        "            raise ValueError(\"max_features must be in (0, n_features)\")\n",
        "        max_features = max(1, int(max_features))\n",
        "        # store validated integer feature sampling values\n",
        "        self._max_features = max_features\n",
        "        self._pop_features = pop_features\n",
        "\n",
        "        # parallel loop\n",
        "        n_jobs, n_estimators_list, starts = _partition_estimators(self.n_estimators, self.n_jobs)\n",
        "        # building estimators\n",
        "        all_results = Parallel(n_jobs=n_jobs)(delayed(_parallel_build_estimators)(\n",
        "                range(starts[i],starts[i+1]), self, p_of_estims_, seeds, X, y, tiebreaker)\n",
        "            for i in range(n_jobs))\n",
        "\n",
        "        self.estimators += list(itertools.chain.from_iterable(\n",
        "            t[0] for t in all_results))\n",
        "        self.estim_features += list(itertools.chain.from_iterable(\n",
        "            t[1] for t in all_results))\n",
        "\n",
        "        if self.min_cq_combination:\n",
        "            predictions = np.zeros((X.shape[0], self.n_estimators))\n",
        "            for k in range(self.n_estimators):\n",
        "                predictions[:, k] = self.estimators[k].predict(X[:, self.estim_features[k]])\n",
        "\n",
        "            from .min_cq import MinCqLearner\n",
        "            mincq = MinCqLearner(self.min_cq_mu, \"stumps\", n_stumps_per_attribute=1,\n",
        "                                 self_complemented=False)\n",
        "            mincq.fit(predictions, y)\n",
        "            self.min_cq_weights = mincq.majority_vote.weights\n",
        "            self.min_cq_weights /= np.sum(np.abs(self.min_cq_weights))\n",
        "\n",
        "        if get_feature_importances:\n",
        "            importances = self.features_importance()\n",
        "            self.feature_importances_ = np.array([importances['avg'][k]\n",
        "                                                  if k in importances['avg'] else 0\n",
        "                                                  for k in range(self.n_features)])\n",
        "            self.feature_importances_ /= np.sum(self.feature_importances_)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Compute model predictions for data in X\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        predictions : array\n",
        "            predictions[i] is the predicted class for he sample i in X\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, [\"estimators\", \"estim_features\"])\n",
        "        X = check_array(X)\n",
        "        predicted_proba = self.predict_proba(X)\n",
        "        predictions = np.array(np.argmax(predicted_proba, axis=1), dtype=int)\n",
        "        predictions = np.array([self.binary_to_labels[l] for l in predictions])\n",
        "        return predictions\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict class probabilities according to the model estimators\n",
        "\n",
        "        Parameters :\n",
        "        ----------\n",
        "        X : array\n",
        "            a dataset to predict\n",
        "\n",
        "        Returns : \n",
        "        ----------\n",
        "        proba : array\n",
        "            proba[c] contains the estimated probability for each sample of X to belong to class c\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, [\"estimators\", \"estim_features\"])\n",
        "        X = check_array(X)\n",
        "        # parallel loop\n",
        "        n_jobs, n_estimators_list, starts = _partition_estimators(self.n_estimators, self.n_jobs)\n",
        "        results = np.zeros(X.shape[0])\n",
        "        results = Parallel(n_jobs=n_jobs)(delayed(_parallel_predict_proba)(\n",
        "                self, X, range(starts[i], starts[i+1]), results)\n",
        "                for i in range(n_jobs))\n",
        "        if not self.min_cq_combination:\n",
        "            votes = sum(results) / self.n_estimators\n",
        "            proba = np.array([np.array([1 - vote, vote]) for vote in votes])\n",
        "        else:\n",
        "            votes = sum(results)\n",
        "            proba = np.array([np.array([(1 - vote)/2, (1 + vote)/2]) for vote in votes])\n",
        "        return proba\n",
        "\n",
        "    def features_importance(self):\n",
        "        \"\"\"\n",
        "        Compute features importances in estimators rules\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        importance : dict\n",
        "            importances['avg'] : dict (feature id as key, mean importance as value)\n",
        "                The mean importance of each feature over the estimators.\n",
        "            importances['max'] : dict (feature id as key, max importance as value)\n",
        "                The maximal importance of each feature over the estimators.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, [\"estimators\", \"estim_features\"])\n",
        "        importances = {'avg' : {}, 'max' : {}} # average and maximal feature/rule importances\n",
        "        feature_id_occurences = {} # number of occurences of a feature in subsamples\n",
        "        for (estim, features_idx) in zip(self.estimators, self.estim_features):\n",
        "            # increment the total occurences of the feature :\n",
        "            for id_feat in features_idx:\n",
        "                if id_feat in feature_id_occurences:\n",
        "                    feature_id_occurences[id_feat] += 1\n",
        "                else:\n",
        "                    feature_id_occurences[id_feat] = 1\n",
        "            # sum the rules importances :\n",
        "            #rules_importances = estim.get_rules_importances() # activate it when pyscm will implement importance\n",
        "            rules_importances = np.ones(len(estim.model_.rules)) #delete it when pyscm will implement importance\n",
        "            for rule, importance in zip(estim.model_.rules, rules_importances):\n",
        "                global_feat_id = features_idx[rule.feature_idx]\n",
        "                if global_feat_id in importances['avg']:\n",
        "                    importances['avg'][global_feat_id] += importance\n",
        "                    if importance > importances['max'][global_feat_id]:\n",
        "                        importances['max'][global_feat_id] = importance\n",
        "                else:\n",
        "                    importances['avg'][global_feat_id] = importance\n",
        "                    importances['max'][global_feat_id] = importance\n",
        "        importances['avg'] = {k: round(v / feature_id_occurences[k], 3) for k, v in importances['avg'].items()}\n",
        "        return importances\n",
        "\n",
        "    def all_data_tiebreaker(self, model_type, feature_idx, thresholds, rule_type, X, y):\n",
        "        \"\"\"\n",
        "        Choose a rule between rule with equal utility\n",
        "        Select the one that have the best accuracy if applied alone on all the data\n",
        "\n",
        "        Parameters :\n",
        "        ----------\n",
        "        model_type : strint ('conjunction' or 'dijunction')\n",
        "            type of the model\n",
        "        feature_idx, thresholds, rule_type : arrays\n",
        "            description of the rules\n",
        "        X, y : arrays\n",
        "            a dataset used to compare rules\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        ID of the rule to select\n",
        "        \"\"\"\n",
        "        keep_id = 0\n",
        "        keep_id_score = -1\n",
        "        for k in range(len(feature_idx)):\n",
        "            feat_id, threshold, r_type = feature_idx[k], thresholds[k], rule_type[k]\n",
        "            stump = DecisionStump(feature_idx=feat_id, threshold=threshold, kind=r_type)\n",
        "            rule_classif = stump.classify(X).astype('int')\n",
        "            rule_global_score = (rule_classif == y).sum()\n",
        "            if rule_global_score > keep_id_score:\n",
        "                keep_id = k\n",
        "                keep_id_score = rule_global_score\n",
        "        return keep_id\n",
        "\n",
        "    def classifiers_disagreement(self, X):\n",
        "        \"\"\"\n",
        "        Compute disagreement between estimators\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        disagreement : dict \n",
        "            disagreement['global'] : int\n",
        "                global disagreement, computed with the following formula :\n",
        "                average(disagreement(c1, c2))\n",
        "                for (c1, c2) all the pairs of distinct estimators in the model\n",
        "                with \n",
        "                disagreement(c1, c2) = (# of examples where c1 and c2 differs)/(# of examples)\n",
        "            disagreement['estims'] : list\n",
        "                list of disagreements of each estimator with the global model\n",
        "            disagreement['heatmap'] : matrix\n",
        "                heatmap of mutual disagreement for each pair of classifiers\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, [\"estimators\", \"estim_features\"])\n",
        "        disagreement = {'global' : 0, 'estims' : []}\n",
        "        total_comp = 0 # number of comparison of estimators\n",
        "        global_pred = self.predict(X)\n",
        "        heatmap = np.zeros((self.n_estimators, self.n_estimators))\n",
        "        for kA in range(len(self.estimators)):\n",
        "            estimA = self.estimators[kA]\n",
        "            predA = estimA.predict(X[:, self.estim_features[kA]])\n",
        "            disagree = np.not_equal(global_pred, predA).sum()/len(X) # disagreement of estimator A and global model\n",
        "            disagreement['estims'].append(disagree)\n",
        "            for kB in range(len(self.estimators)):\n",
        "                if kB == kA:\n",
        "                    continue # do not compare the estimator with itself\n",
        "                estimB = self.estimators[kB]\n",
        "                predB = estimB.predict(X[:, self.estim_features[kB]])\n",
        "                disagree = np.not_equal(predB, predA).sum()/len(X) # disagreement of estimators A and B\n",
        "                heatmap[kA][kB] = disagree\n",
        "                disagreement['global'] += disagree\n",
        "                total_comp += 1\n",
        "        disagreement['global'] = disagreement['global']/total_comp\n",
        "        disagreement['heatmap'] = heatmap\n",
        "        return disagreement\n",
        "    \n",
        "    def get_estimators_indices(self):\n",
        "        \"\"\"\n",
        "        Get drawn indices along both sample and feature axes\n",
        "        \"\"\"\n",
        "        for seed in self._seeds:\n",
        "            # operations accessing random_state must be performed identically to those in 'fit'\n",
        "            feature_indices = sample_without_replacement(self._pop_features, self._max_features, random_state=seed)\n",
        "            samples_indices = sample_without_replacement(self._pop_samples, self._max_samples, random_state=seed)\n",
        "            yield samples_indices\n",
        "\n",
        "    def score(self, X, y):\n",
        "        check_is_fitted(self, [\"estimators\", \"estim_features\"])\n",
        "        X, y = check_X_y(X, y)\n",
        "        return accuracy_score(y, self.predict(X))"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZIeKa2fzSwd",
        "outputId": "a8a192ba-6c59-4b7b-c147-6689002da7b9"
      },
      "source": [
        "pip install pyscm-ml"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyscm-ml\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/0c/f7fab775e1a2c01da5ff45d563d62021e878926decb04cb061047ccb144a/pyscm-ml-1.0.3.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyscm-ml) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyscm-ml) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyscm-ml) (0.22.2.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pyscm-ml) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyscm-ml) (1.0.1)\n",
            "Building wheels for collected packages: pyscm-ml\n",
            "  Building wheel for pyscm-ml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyscm-ml: filename=pyscm_ml-1.0.3-cp37-cp37m-linux_x86_64.whl size=70276 sha256=328d5e1e2beea7a280958138f87c3194636b7b08195bb295fca333b16db2ba4c\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/95/01/a8cb6672e16a9564706234568f8f4d8707d6dab30e5f0debb3\n",
            "Successfully built pyscm-ml\n",
            "Installing collected packages: pyscm-ml\n",
            "Successfully installed pyscm-ml-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV-ZH6dx5SpV"
      },
      "source": [
        "result = RandomScmClassifier()"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJQioB8S8nx5",
        "outputId": "5e07d90e-2953-4467-83c3-bdd8ea4988ba"
      },
      "source": [
        "from pprint import pprint\n",
        "# Look at parameters used by our current forest\n",
        "print('Parameters currently in use:\\n')\n",
        "pprint(result.get_params())"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameters currently in use:\n",
            "\n",
            "{'max_features': 0.5,\n",
            " 'max_rules': 10,\n",
            " 'max_samples': 0.5,\n",
            " 'min_cq_combination': False,\n",
            " 'min_cq_mu': 0.01,\n",
            " 'model_type': 'conjunction',\n",
            " 'n_estimators': 100,\n",
            " 'n_jobs': None,\n",
            " 'p_options': [1.0],\n",
            " 'random_state': None}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHovD0Ge8wQ4",
        "outputId": "7a013ae1-8ba7-4645-ac82-ff60b06fbc5d"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# Number of trees in random forest\n",
        "n_estimators = [1, 10, 100, 1000] #[int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = [0.25, 0.5, 0.75, 1.0] #['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_samples = [0.25, 0.5, 0.75, 1.0] #[int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "#max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "p_options = [[0.1], [0.5], [1.0], [5.0], [10.0]]\n",
        "# Minimum number of samples required at each leaf node\n",
        "model_type = ['conjunction', 'disjunction']\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_samples': max_samples,\n",
        "               'p_options': p_options,\n",
        "               'model_type': model_type}\n",
        "pprint(random_grid)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'max_features': [0.25, 0.5, 0.75, 1.0],\n",
            " 'max_samples': [0.25, 0.5, 0.75, 1.0],\n",
            " 'model_type': ['conjunction', 'disjunction'],\n",
            " 'n_estimators': [1, 10, 100, 1000],\n",
            " 'p_options': [[0.1], [0.5], [1.0], [5.0], [10.0]]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxuKFY0aKNQf",
        "outputId": "40c5831b-72fc-4c85-c5a5-82822c9b8d33"
      },
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "#rf = RandomForestClassifier()\n",
        "result = RandomScmClassifier()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "result = RandomizedSearchCV(estimator = result, param_distributions = random_grid, n_iter = 200, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "result.fit(X_train, y_train)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  8.3min\n",
            "[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed: 14.9min\n",
            "[Parallel(n_jobs=-1)]: Done 644 tasks      | elapsed: 25.8min\n",
            "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 34.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=5, error_score=nan,\n",
              "                   estimator=RandomScmClassifier(max_features=0.5, max_rules=10,\n",
              "                                                 max_samples=0.5,\n",
              "                                                 min_cq_combination=False,\n",
              "                                                 min_cq_mu=0.01,\n",
              "                                                 model_type='conjunction',\n",
              "                                                 n_estimators=100, n_jobs=None,\n",
              "                                                 p_options=[1.0],\n",
              "                                                 random_state=None),\n",
              "                   iid='deprecated', n_iter=200, n_jobs=-1,\n",
              "                   param_distributions={'max_features': [0.25, 0.5, 0.75, 1.0],\n",
              "                                        'max_samples': [0.25, 0.5, 0.75, 1.0],\n",
              "                                        'model_type': ['conjunction',\n",
              "                                                       'disjunction'],\n",
              "                                        'n_estimators': [1, 10, 100, 1000],\n",
              "                                        'p_options': [[0.1], [0.5], [1.0],\n",
              "                                                      [5.0], [10.0]]},\n",
              "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wicxRiJ0TIje",
        "outputId": "6d0df46f-0e16-41a6-efee-c1557602a72c"
      },
      "source": [
        "result.best_params_"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_features': 0.5,\n",
              " 'max_samples': 0.5,\n",
              " 'model_type': 'disjunction',\n",
              " 'n_estimators': 10,\n",
              " 'p_options': [1.0]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOcyLM0H1YVS",
        "outputId": "d9c67129-43d3-4084-f7a3-e66836507ca7"
      },
      "source": [
        "print(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 0.0 ... 1084.074486 91004.26917 -145.43122140000003]\n",
            " [0.0 0.0 0.0 ... 1106.098664 100736.478 -170.776811]\n",
            " [0.0 0.0 0.0 ... 1122.8278990000001 94610.99892 -140.8011661]\n",
            " ...\n",
            " [0.0 0.0 0.0 ... 1024.050104 70873.17068 218.3647556]\n",
            " [0.0 0.0 0.0 ... 1107.663733 72563.38294 234.9694451]\n",
            " [0.0 0.0 0.0 ... 1049.796854 72014.0392 216.1274312]] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI2g6UKD1V78"
      },
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmrvlJmm1lAK",
        "outputId": "6d0db5b1-6ba9-48fd-9d72-90c4d33dc3d2"
      },
      "source": [
        "print(X_train, y_train)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0 0.0 0.0 ... 5.006445885 -499.59357760000006 0.0]\n",
            " [0.0 0.0 0.0 ... 4.937784848 -597.6507706 0.222810975]\n",
            " [0.0 0.0 0.0 ... 3.6159958889999997 -339.9211588 0.141536156]\n",
            " ...\n",
            " [0.0 0.0 0.0 ... 9.386030279 -382.17366489999995 0.458696214]\n",
            " [0.0 0.0 0.0 ... 2.697759413 -328.980506 0.0]\n",
            " [0.0 0.0 0.0 ... 5.560928304 -783.5850869 0.367443491]] [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP9TB36qJ6qN"
      },
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=1) # 0.125 x 0.8 = 0.10\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 1)\n",
        "#from sklearn.ensemble import RandomForestClassifier\n",
        "#clf=RandomForestClassifier(n_estimators=100)\n",
        "#print(X_test[0])"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO9fAL45PveR"
      },
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn import metrics"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhMzs_dmPyWn"
      },
      "source": [
        "#### Bootstrapping ####\n",
        "########################################################\n",
        "# Creating empty list to hold accuracy values\n",
        "AccuracyValues=[]\n",
        "n_times=60"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq6xqiFNP06f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "582c028c-9754-4f0f-91d1-772c9f170971"
      },
      "source": [
        "## Performing bootstrapping\n",
        "from sklearn import metrics\n",
        "for i in range(n_times):\n",
        "    #Split the data into training and testing set\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    # Changing the seed value for each iteration\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42+i)\n",
        "    result = RandomScmClassifier(max_features=0.5, n_estimators=10, max_samples=0.5, model_type='disjunction', p_options=[1.0])\n",
        "    result.fit(X_train, y_train)\n",
        "    ########################################################\n",
        "    prediction = result.predict(X_test)\n",
        "    Accuracy=metrics.accuracy_score(y_test, prediction)\n",
        "    print(Accuracy)\n",
        "    AccuracyValues.append((Accuracy))\n",
        "    #print(Accuracy)\n",
        "    print(AccuracyValues)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "[1.0]\n",
            "1.0\n",
            "[1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "0.9\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "0.9\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "0.9\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0]\n",
            "0.9\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "0.9\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9]\n",
            "0.9\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "0.9\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0]\n",
            "1.0\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QJjZEWAP4ef",
        "outputId": "61a996f3-57c2-4fdc-9ea0-9b6fe4c95fe7"
      },
      "source": [
        "###### Single Decision Tree Regression in Python #######\n",
        "    #choose from different tunable hyper parameters\n",
        "    #RegModel = tree.DecisionTreeRegressor(max_depth=3,criterion='mse')\n",
        " \n",
        "    #Creating the model on Training Data\n",
        "    #DTree=RegModel.fit(X_train,y_train)\n",
        "    #prediction=DTree.predict(X_test)\n",
        " \n",
        "    #Measuring accuracy on Testing Data\n",
        "#Accuracy=100- (np.mean(np.abs((y_test - prediction) / y_test)) * 100)\n",
        "    \n",
        "    # Storing accuracy values\n",
        "#AccuracyValues.append(np.round(Accuracy))\n",
        "    \n",
        "################################################\n",
        "# Result of all bootstrapping trials\n",
        "print(AccuracyValues)\n",
        " \n",
        "# Final accuracy\n",
        "print('Final average accuracy',np.mean(AccuracyValues))\n",
        "#print(\"Test Accuracy:\", metrics.accuracy_score(y_test, y_final ))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "Final average accuracy 0.9883333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmUj02i1QBuk",
        "outputId": "b05688a5-8581-4c92-b225-21204ed30887"
      },
      "source": [
        "print(prediction)\n",
        "print(prediction.astype(int).sum())\n",
        "print(len(y_test))\n",
        "#(prediction).astype(int).sum()/len(y_test)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 0 0 1 0 0 1 1]\n",
            "6\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCVT0xeJUxDJ"
      },
      "source": [
        "result = RandomScmClassifier(max_features=0.5, n_estimators=10, max_samples=0.5, model_type='disjunction', p_options=[1.0])"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SjrzgfNjX3b"
      },
      "source": [
        "result = RandomScmClassifier()"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loSDBFEw0i__"
      },
      "source": [
        "result.fit(X_train, y_train)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI3elXul61BW",
        "outputId": "20f3a9c5-876d-487f-988e-365c78290200"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "predictions = result.predict(X_test)\n",
        "#result.accuracy_score(y_test, predictions)\n",
        "#test = metrics.accuracy_score(y_test, predictions)\n",
        "print(\"Test Accuracy:\", metrics.accuracy_score(y_test, predictions))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "ihDSlrVgeYmn",
        "outputId": "da364173-e2c0-46e9-e92c-07b46c41325e"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1_score(y_test, predictions)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-a001d6704a5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1097\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m                        \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                        zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1224\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m                                                  \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m                                                  zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1227\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1484\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                          str(average_options))\n\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \"\"\"\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [10, 30]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "n4oRLT72fYYi",
        "outputId": "a3f2f772-f701-4232-ed9b-833a9533a81f"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "roc_auc_score(y, result.predict_proba(X), multi_class='ovr')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-dc1ba501fc38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ovr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    388\u001b[0m                                              max_fpr=max_fpr),\n\u001b[1;32m    389\u001b[0m                                      \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                                      sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    391\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# multilabel-indicator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         return _average_binary_score(partial(_binary_roc_auc_score,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     fpr, tpr, _ = roc_curve(y_true, y_score,\n\u001b[0;32m--> 225\u001b[0;31m                             sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \"\"\"\n\u001b[1;32m    770\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 771\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m     \u001b[0massert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[0massert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad input shape {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: bad input shape (100, 2)"
          ]
        }
      ]
    }
  ]
}