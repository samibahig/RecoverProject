{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RandomSCMProteomique",
      "provenance": [],
      "authorship_tag": "ABX9TyOa8hxn3syGUtb4H1+Te2ex",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samibahig/RecoverProject/blob/main/RandomSCMProteomique_Cyst.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYYBb-wpqfLT",
        "outputId": "09bdeb67-579e-4d68-d245-b566598015ca"
      },
      "source": [
        "### Ici on download les données, on les nettoie et on les met sous forme de matrices: proteomics pour les features et metadata pour les labels, \n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle as pkl\n",
        "#print('\\nMETADATA :')\n",
        "data_path = ''\n",
        "metadata_filename = '/content/metadata.csv'\n",
        "meta_df = pd.read_csv(metadata_filename)\n",
        "print('-----------')\n",
        "print(meta_df)\n",
        "print('-----------')\n",
        "#print(meta_df.columns)\n",
        "meta_df.columns = ['#', 'plate', '-', 'symptoms'] + list(meta_df)[4:]\n",
        "#print(meta_df.columns)\n",
        "#print('available metadata :', list(meta_df))\n",
        "meta_idx = meta_df['ID'].to_list()\n",
        "meta_label = meta_df['symptoms'].to_list()\n",
        "#print('------------------')\n",
        "#print(list(zip(meta_idx, meta_label)))\n",
        "#print('------------------')\n",
        "meta_id_label_dict = {str(k): 1 if v=='S' else 0 for k, v in zip(meta_idx, meta_label)}\n",
        "\n",
        "#DF1 : proteomics\n",
        "#print('\\nPROEOMICS DATA :')\n",
        "proteomics_data_filename = '/content/proteomics_cyt.csv'\n",
        "\n",
        "dim_df = pd.read_csv(proteomics_data_filename, nrows=1)\n",
        "#print('--------')\n",
        "#print(dim_df)\n",
        "#print('--------')\n",
        "dim = len(list(dim_df))\n",
        "#print(dim)\n",
        "#print('------------')\n",
        "print('# of columns in source csv file :', dim)\n",
        "all_cols = [i for i in range(dim)]\n",
        "\n",
        "print('--------')\n",
        "feat_cols = all_cols[1:-4]\n",
        "print(feat_cols)\n",
        "print('--------')\n",
        "samplesidx_col = [0]\n",
        "\n",
        "feat_df = pd.read_csv(proteomics_data_filename, skiprows=4, nrows=1, dtype=str, usecols=feat_cols)\n",
        "features = list(feat_df)\n",
        "print('# of features : ', len(features))\n",
        "print('first feature :', features[0])\n",
        "print('last feature :', features[-1])\n",
        "\n",
        "idx_df = pd.read_csv(proteomics_data_filename, skiprows=6, index_col=0, skipfooter=4, usecols=[0], engine='python')\n",
        "idx = list(idx_df.index.values)\n",
        "print('# of idx : ', len(idx))\n",
        "print('first id :', idx[0])\n",
        "print('last id :', idx[-1])\n",
        "\n",
        "df1 = pd.read_csv(proteomics_data_filename, skiprows=6, skipfooter=4, usecols=feat_cols, engine='python')\n",
        "df1.fillna(value=0, axis=1, inplace=True, limit=None)\n",
        "assert df1.shape[0] == len(idx)\n",
        "assert df1.shape[1] == len(features)\n",
        "\n",
        "df1['idx'] = idx\n",
        "df1.set_index('idx', inplace=True)\n",
        "df1.columns = features\n",
        "print('# of Nan values :', df1.isna().sum().sum())\n",
        "\n",
        "#clean data of samples that are not in metadata :\n",
        "idx = df1.index.values\n",
        "y = []\n",
        "for k in range(len(idx)):\n",
        "    id = idx[k]\n",
        "    if id in meta_id_label_dict:\n",
        "        y.append(meta_id_label_dict[id])\n",
        "    else:\n",
        "        # we will not put this sample in the dataset\n",
        "        #print('sample to remove because of unknown label:', k, id)\n",
        "        y.append('to_remove')\n",
        "df1['label'] = y\n",
        "df1 = df1[df1.label != 'to_remove']\n",
        "\n",
        "#create X and y matrices for ML :\n",
        "y = list(df1['label'])\n",
        "del df1['label']\n",
        "print('---------')\n",
        "df1.drop(columns=['P01133'], inplace=True)\n",
        "#df1.drop('3-009', inplace=True)\n",
        "print(df1)\n",
        "print('---------')\n",
        "X = df1.to_numpy()\n",
        "print('proteomics_cyst data :')\n",
        "print('# of samples : ', df1.shape[0])\n",
        "print('# of features : ', df1.shape[1])\n",
        "print('labels:', list(dict.fromkeys(y)))\n",
        "\n",
        "\n",
        "## save X and y in pickles if you want :\n",
        "##data_name = 'recover_multiomics_'\n",
        "##feat_dict = {k: str(v) for k, v in zip(range(len(list(df))), list(df))}\n",
        "##with open(data_path + data_name + 'feat_dict', 'wb') as fo:\n",
        "##    pkl.dump(feat_dict, fo)\n",
        "##with open(data_path + data_name + 'X', 'wb') as fo:df\n",
        "##            pkl.dump(X, fo)\n",
        "##with open(data_path + data_name + 'y', 'wb') as fo:\n",
        "##            pkl.dump(y, fo)\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------\n",
            "    Unnamed: 0 Unnamed: 1  ... addition                                specify.1\n",
            "0            1    Plate 1  ...      NaN                                      NaN\n",
            "1            2    Plate 1  ...      5.0                                   Stress\n",
            "2            3    Plate 1  ...      6.0  back pain is worse, shortness of breath\n",
            "3            4    Plate 1  ...      2.0  Impression of a foreingh body in throat\n",
            "4            5    Plate 1  ...      2.0                                      NaN\n",
            "..         ...        ...  ...      ...                                      ...\n",
            "95         100    Plate 2  ...      NaN                                      NaN\n",
            "96         101    Plate 2  ...      NaN                                      NaN\n",
            "97         102    Plate 2  ...      NaN                                      NaN\n",
            "98         103    Plate 2  ...      NaN                                      NaN\n",
            "99         104    Plate 2  ...      NaN                                      NaN\n",
            "\n",
            "[100 rows x 46 columns]\n",
            "-----------\n",
            "# of columns in source csv file : 48\n",
            "--------\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43]\n",
            "--------\n",
            "# of features :  43\n",
            "first feature : P80075\n",
            "last feature : P01374\n",
            "# of idx :  120\n",
            "first id : nan\n",
            "last id : Plate LOD\n",
            "# of Nan values : 0\n",
            "---------\n",
            "          P80075   O95760     P48061  ...     P13236    O14625    P01374\n",
            "idx                                   ...                               \n",
            "5-139   32.88848  0.55984  397.29429  ...   91.57302   45.7468  18.96545\n",
            "1-039   81.61041  0.06288  340.58883  ...  195.10538  35.51699  11.14506\n",
            "1-062   21.05386  0.21039   269.3803  ...  307.71337  45.26911   7.55923\n",
            "1-040   65.32818  0.17781  213.10409  ...  133.96489  62.91014  14.23622\n",
            "2-044   85.72217  0.08509  274.67257  ...  213.41426   48.2183    8.3883\n",
            "...          ...      ...        ...  ...        ...       ...       ...\n",
            "3-043   51.63355  0.03683  270.87338  ...   133.8363  59.51365   8.67786\n",
            "2-106   93.33449  0.37065  340.32447  ...  146.29085  43.38619   9.16696\n",
            "5-078  137.76807        0  283.83411  ...  128.55601  41.31431   9.29271\n",
            "3-021   74.34012  0.19809  241.85155  ...   132.4548  51.90362   6.99919\n",
            "3-045   62.24544  0.01084  241.12205  ...    96.2235  30.74631   9.71401\n",
            "\n",
            "[100 rows x 42 columns]\n",
            "---------\n",
            "proteomics_cyst data :\n",
            "# of samples :  100\n",
            "# of features :  42\n",
            "labels: [1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es7Pav1Xf3c8"
      },
      "source": [
        "#df1.tail(20)\n",
        "#df1['P01133']\n",
        "#\n",
        "#df1.drop([df1['P01133'] == '> ULOQ'], inplace = True)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKvpe6Cp_x7X",
        "outputId": "8e9c44c8-b6b3-4006-8488-6b1f5811b487",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df1.columns"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['P80075', 'O95760', 'P48061', 'P78380', 'Q8NEV9_Q14213', 'P60568',\n",
              "       'Q07325', 'P01135', 'P01584', 'P05231', 'P05112', 'O43508', 'Q969D9',\n",
              "       'P51671', 'P14210', 'P49771', 'Q96PD4', 'P13232', 'P35225', 'Q14116',\n",
              "       'Q99616', 'P50591', 'P02778', 'P01579', 'P22301', 'Q99731', 'P01375',\n",
              "       'P40933', 'P10147', 'P10145', 'P39900', 'P04141', 'P09919', 'P15692',\n",
              "       'Q9P0M4', 'P13500', 'Q16552', 'P13725', 'P09603', 'P13236', 'O14625',\n",
              "       'P01374'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xO7MRl3X3WH",
        "outputId": "468c831e-6654-4fcf-9985-7b309d7fcb5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "df1.fillna(value=0, axis=1, inplace=True, limit=None)\n",
        "df1.head(50)\n",
        "#df1 = df1.drop(labels='> ULOQ', axis=1, inplace=True)\n",
        "df1"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pg/mL</th>\n",
              "      <th>pg/mL.1</th>\n",
              "      <th>pg/mL.2</th>\n",
              "      <th>pg/mL.3</th>\n",
              "      <th>pg/mL.4</th>\n",
              "      <th>pg/mL.5</th>\n",
              "      <th>pg/mL.6</th>\n",
              "      <th>pg/mL.7</th>\n",
              "      <th>pg/mL.8</th>\n",
              "      <th>pg/mL.9</th>\n",
              "      <th>pg/mL.10</th>\n",
              "      <th>pg/mL.11</th>\n",
              "      <th>pg/mL.12</th>\n",
              "      <th>pg/mL.13</th>\n",
              "      <th>pg/mL.14</th>\n",
              "      <th>pg/mL.15</th>\n",
              "      <th>pg/mL.16</th>\n",
              "      <th>pg/mL.17</th>\n",
              "      <th>pg/mL.18</th>\n",
              "      <th>pg/mL.19</th>\n",
              "      <th>pg/mL.20</th>\n",
              "      <th>pg/mL.21</th>\n",
              "      <th>pg/mL.22</th>\n",
              "      <th>pg/mL.23</th>\n",
              "      <th>pg/mL.24</th>\n",
              "      <th>pg/mL.25</th>\n",
              "      <th>pg/mL.26</th>\n",
              "      <th>pg/mL.27</th>\n",
              "      <th>pg/mL.28</th>\n",
              "      <th>pg/mL.29</th>\n",
              "      <th>pg/mL.30</th>\n",
              "      <th>pg/mL.31</th>\n",
              "      <th>pg/mL.32</th>\n",
              "      <th>pg/mL.33</th>\n",
              "      <th>pg/mL.34</th>\n",
              "      <th>pg/mL.35</th>\n",
              "      <th>pg/mL.36</th>\n",
              "      <th>pg/mL.37</th>\n",
              "      <th>pg/mL.38</th>\n",
              "      <th>pg/mL.39</th>\n",
              "      <th>pg/mL.40</th>\n",
              "      <th>pg/mL.41</th>\n",
              "      <th>pg/mL.42</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>32.88848</td>\n",
              "      <td>0.55984</td>\n",
              "      <td>397.29429</td>\n",
              "      <td>146.91341</td>\n",
              "      <td>1.49025</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>47.63594</td>\n",
              "      <td>19.60223</td>\n",
              "      <td>0.01838</td>\n",
              "      <td>4.08696</td>\n",
              "      <td>0.02049</td>\n",
              "      <td>841.61841</td>\n",
              "      <td>0.02052</td>\n",
              "      <td>120.03389</td>\n",
              "      <td>655.88138</td>\n",
              "      <td>180.05744</td>\n",
              "      <td>0.04859</td>\n",
              "      <td>5.54881</td>\n",
              "      <td>20.82823</td>\n",
              "      <td>263.5005</td>\n",
              "      <td>122.60355</td>\n",
              "      <td>367.37495</td>\n",
              "      <td>107.92546</td>\n",
              "      <td>0.30287</td>\n",
              "      <td>1.3686</td>\n",
              "      <td>130.27204</td>\n",
              "      <td>19.46435</td>\n",
              "      <td>11.99858</td>\n",
              "      <td>7.40087</td>\n",
              "      <td>10.74015</td>\n",
              "      <td>185.29854</td>\n",
              "      <td>0.28222</td>\n",
              "      <td>84.90633</td>\n",
              "      <td>460.05592</td>\n",
              "      <td>7.50828</td>\n",
              "      <td>440.98377</td>\n",
              "      <td>546.78923</td>\n",
              "      <td>0.18685</td>\n",
              "      <td>11.57456</td>\n",
              "      <td>169.21768</td>\n",
              "      <td>91.57302</td>\n",
              "      <td>45.7468</td>\n",
              "      <td>18.96545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>81.61041</td>\n",
              "      <td>0.06288</td>\n",
              "      <td>340.58883</td>\n",
              "      <td>513.65527</td>\n",
              "      <td>2.62802</td>\n",
              "      <td>0.01274</td>\n",
              "      <td>20.77929</td>\n",
              "      <td>36.61691</td>\n",
              "      <td>0.13591</td>\n",
              "      <td>7.68976</td>\n",
              "      <td>0.0522</td>\n",
              "      <td>685.44529</td>\n",
              "      <td>0.02987</td>\n",
              "      <td>120.02688</td>\n",
              "      <td>1072.31251</td>\n",
              "      <td>119.73066</td>\n",
              "      <td>0.47567</td>\n",
              "      <td>4.10289</td>\n",
              "      <td>0.29985</td>\n",
              "      <td>246.075</td>\n",
              "      <td>186.67814</td>\n",
              "      <td>396.55929</td>\n",
              "      <td>51.91873</td>\n",
              "      <td>0.12844</td>\n",
              "      <td>0.59468</td>\n",
              "      <td>99.51079</td>\n",
              "      <td>10.33548</td>\n",
              "      <td>10.15531</td>\n",
              "      <td>6.6197</td>\n",
              "      <td>11.14864</td>\n",
              "      <td>154.31861</td>\n",
              "      <td>0.15098</td>\n",
              "      <td>97.96295</td>\n",
              "      <td>427.58919</td>\n",
              "      <td>12.9628</td>\n",
              "      <td>490.3056</td>\n",
              "      <td>466.59691</td>\n",
              "      <td>0.15477</td>\n",
              "      <td>15.16106</td>\n",
              "      <td>133.40028</td>\n",
              "      <td>195.10538</td>\n",
              "      <td>35.51699</td>\n",
              "      <td>11.14506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>21.05386</td>\n",
              "      <td>0.21039</td>\n",
              "      <td>269.3803</td>\n",
              "      <td>195.82065</td>\n",
              "      <td>6.40496</td>\n",
              "      <td>0.02725</td>\n",
              "      <td>59.49787</td>\n",
              "      <td>19.51414</td>\n",
              "      <td>0.13468</td>\n",
              "      <td>3.96441</td>\n",
              "      <td>0.04595</td>\n",
              "      <td>824.84789</td>\n",
              "      <td>0.03021</td>\n",
              "      <td>98.03313</td>\n",
              "      <td>630.98303</td>\n",
              "      <td>180.63118</td>\n",
              "      <td>3.59341</td>\n",
              "      <td>4.22493</td>\n",
              "      <td>0.30544</td>\n",
              "      <td>197.57757</td>\n",
              "      <td>165.57297</td>\n",
              "      <td>611.68764</td>\n",
              "      <td>91.07628</td>\n",
              "      <td>0.23817</td>\n",
              "      <td>0.74935</td>\n",
              "      <td>114.361</td>\n",
              "      <td>21.53479</td>\n",
              "      <td>11.63106</td>\n",
              "      <td>8.86735</td>\n",
              "      <td>9.76674</td>\n",
              "      <td>221.43462</td>\n",
              "      <td>0.24339</td>\n",
              "      <td>46.79207</td>\n",
              "      <td>540.76521</td>\n",
              "      <td>32.22662</td>\n",
              "      <td>572.97188</td>\n",
              "      <td>528.97433</td>\n",
              "      <td>0.71538</td>\n",
              "      <td>7.18436</td>\n",
              "      <td>130.32299</td>\n",
              "      <td>307.71337</td>\n",
              "      <td>45.26911</td>\n",
              "      <td>7.55923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>65.32818</td>\n",
              "      <td>0.17781</td>\n",
              "      <td>213.10409</td>\n",
              "      <td>221.91738</td>\n",
              "      <td>3.62822</td>\n",
              "      <td>0.02938</td>\n",
              "      <td>87.22272</td>\n",
              "      <td>8.42985</td>\n",
              "      <td>0.03051</td>\n",
              "      <td>5.64413</td>\n",
              "      <td>0.01686</td>\n",
              "      <td>704.85085</td>\n",
              "      <td>0.01658</td>\n",
              "      <td>170.40823</td>\n",
              "      <td>466.69849</td>\n",
              "      <td>115.24552</td>\n",
              "      <td>4.90308</td>\n",
              "      <td>6.883</td>\n",
              "      <td>0.63191</td>\n",
              "      <td>259.29947</td>\n",
              "      <td>194.00077</td>\n",
              "      <td>774.57014</td>\n",
              "      <td>172.86498</td>\n",
              "      <td>0.52656</td>\n",
              "      <td>0.95139</td>\n",
              "      <td>174.72671</td>\n",
              "      <td>21.80639</td>\n",
              "      <td>13.05225</td>\n",
              "      <td>15.51203</td>\n",
              "      <td>12.51854</td>\n",
              "      <td>358.06817</td>\n",
              "      <td>0.26974</td>\n",
              "      <td>85.96804</td>\n",
              "      <td>502.53571</td>\n",
              "      <td>8.03254</td>\n",
              "      <td>321.30365</td>\n",
              "      <td>609.9247</td>\n",
              "      <td>0.22472</td>\n",
              "      <td>3.80081</td>\n",
              "      <td>138.49611</td>\n",
              "      <td>133.96489</td>\n",
              "      <td>62.91014</td>\n",
              "      <td>14.23622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>0.029802322</td>\n",
              "      <td>0.238418579</td>\n",
              "      <td>30.51757809</td>\n",
              "      <td>0.953674317</td>\n",
              "      <td>0.238418579</td>\n",
              "      <td>0.11920929</td>\n",
              "      <td>0.11920929</td>\n",
              "      <td>0.476837158</td>\n",
              "      <td>0.190734863</td>\n",
              "      <td>0.059604645</td>\n",
              "      <td>0.059604645</td>\n",
              "      <td>3.814697264</td>\n",
              "      <td>0.476837158</td>\n",
              "      <td>0.238418579</td>\n",
              "      <td>0.238418579</td>\n",
              "      <td>0.953674318</td>\n",
              "      <td>0.461511623</td>\n",
              "      <td>0.238418579</td>\n",
              "      <td>0.476837158</td>\n",
              "      <td>0.953674317</td>\n",
              "      <td>0.059604645</td>\n",
              "      <td>0.953674317</td>\n",
              "      <td>0.11920929</td>\n",
              "      <td>0.029802322</td>\n",
              "      <td>0.21826022</td>\n",
              "      <td>0.953674318</td>\n",
              "      <td>3.814697267</td>\n",
              "      <td>0.11920929</td>\n",
              "      <td>0.029802322</td>\n",
              "      <td>0.238418579</td>\n",
              "      <td>7.629394533</td>\n",
              "      <td>0.238418579</td>\n",
              "      <td>7.62939454</td>\n",
              "      <td>0.476837158</td>\n",
              "      <td>1.90734863</td>\n",
              "      <td>0.476837158</td>\n",
              "      <td>0.238418579</td>\n",
              "      <td>0.11920929</td>\n",
              "      <td>0.11920929</td>\n",
              "      <td>0.238418579</td>\n",
              "      <td>0.11920929</td>\n",
              "      <td>0.11920929</td>\n",
              "      <td>0.11920929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>0.029802322</td>\n",
              "      <td>0.238418579</td>\n",
              "      <td>30.51757809</td>\n",
              "      <td>0.953674317</td>\n",
              "      <td>0.238418579</td>\n",
              "      <td>0.11920929</td>\n",
              "      <td>0.11920929</td>\n",
              "      <td>0.476837158</td>\n",
              "      <td>0.190734863</td>\n",
              "      <td>0.059604645</td>\n",
              "      <td>0.059604645</td>\n",
              "      <td>3.814697264</td>\n",
              "      <td>0.476837158</td>\n",
              "      <td>0.238418579</td>\n",
              "      <td>0.238418579</td>\n",
              "      <td>0.953674318</td>\n",
              "      <td>0.336278806</td>\n",
              "      <td>0.238418579</td>\n",
              "      <td>0.476837158</td>\n",
              "      <td>0.953674317</td>\n",
              "      <td>0.059604645</td>\n",
              "      <td>0.953674317</td>\n",
              "      <td>0.11920929</td>\n",
              "      <td>0.029802322</td>\n",
              "      <td>0.164854579</td>\n",
              "      <td>0.953674318</td>\n",
              "      <td>3.814697267</td>\n",
              "      <td>0.11920929</td>\n",
              "      <td>0.029802322</td>\n",
              "      <td>0.238418579</td>\n",
              "      <td>7.629394533</td>\n",
              "      <td>0.238418579</td>\n",
              "      <td>7.62939454</td>\n",
              "      <td>0.476837158</td>\n",
              "      <td>1.90734863</td>\n",
              "      <td>0.476837158</td>\n",
              "      <td>0.238418579</td>\n",
              "      <td>0.11920929</td>\n",
              "      <td>0.11920929</td>\n",
              "      <td>0.238418579</td>\n",
              "      <td>0.11920929</td>\n",
              "      <td>0.11920929</td>\n",
              "      <td>0.11920929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>0.00654322</td>\n",
              "      <td>0.090658947</td>\n",
              "      <td>22.5492884</td>\n",
              "      <td>0</td>\n",
              "      <td>0.039103235</td>\n",
              "      <td>0.003278328</td>\n",
              "      <td>0</td>\n",
              "      <td>0.380296159</td>\n",
              "      <td>0.035011025</td>\n",
              "      <td>0.00623886</td>\n",
              "      <td>0.018508553</td>\n",
              "      <td>0</td>\n",
              "      <td>0.080167425</td>\n",
              "      <td>0.055941269</td>\n",
              "      <td>0.130523906</td>\n",
              "      <td>0.04623328</td>\n",
              "      <td>0.681152304</td>\n",
              "      <td>0.151134977</td>\n",
              "      <td>0.042896645</td>\n",
              "      <td>0.088227115</td>\n",
              "      <td>0.003589045</td>\n",
              "      <td>0.002936432</td>\n",
              "      <td>0</td>\n",
              "      <td>0.008157265</td>\n",
              "      <td>0.368959649</td>\n",
              "      <td>0.368654726</td>\n",
              "      <td>1.793279401</td>\n",
              "      <td>0.154110138</td>\n",
              "      <td>0.011923965</td>\n",
              "      <td>0.034521</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5.678050358</td>\n",
              "      <td>0.010908219</td>\n",
              "      <td>0.633173316</td>\n",
              "      <td>0.048226292</td>\n",
              "      <td>0.015506466</td>\n",
              "      <td>0.051797935</td>\n",
              "      <td>0</td>\n",
              "      <td>0.07180584</td>\n",
              "      <td>0</td>\n",
              "      <td>0.038301252</td>\n",
              "      <td>0.120293123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>0.003132865</td>\n",
              "      <td>0.031866869</td>\n",
              "      <td>13.10544896</td>\n",
              "      <td>0</td>\n",
              "      <td>0.045446061</td>\n",
              "      <td>0.003339611</td>\n",
              "      <td>0</td>\n",
              "      <td>0.362257312</td>\n",
              "      <td>0.056722801</td>\n",
              "      <td>0.004342629</td>\n",
              "      <td>0.018912309</td>\n",
              "      <td>0</td>\n",
              "      <td>0.159709352</td>\n",
              "      <td>0.063818236</td>\n",
              "      <td>0.056850042</td>\n",
              "      <td>0</td>\n",
              "      <td>0.461511623</td>\n",
              "      <td>0.143669181</td>\n",
              "      <td>0.03346799</td>\n",
              "      <td>0.090372695</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.011990984</td>\n",
              "      <td>0.21826022</td>\n",
              "      <td>0.329804173</td>\n",
              "      <td>2.531876262</td>\n",
              "      <td>0.073572051</td>\n",
              "      <td>0.006164947</td>\n",
              "      <td>0.033517107</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.337840008</td>\n",
              "      <td>0.009423031</td>\n",
              "      <td>0.639602008</td>\n",
              "      <td>0.092627351</td>\n",
              "      <td>0</td>\n",
              "      <td>0.061460904</td>\n",
              "      <td>0</td>\n",
              "      <td>0.057046267</td>\n",
              "      <td>0</td>\n",
              "      <td>0.049703948</td>\n",
              "      <td>0.073873469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>0</td>\n",
              "      <td>0.031234313</td>\n",
              "      <td>19.0730275</td>\n",
              "      <td>0</td>\n",
              "      <td>0.051784007</td>\n",
              "      <td>0.004719683</td>\n",
              "      <td>0</td>\n",
              "      <td>0.387509348</td>\n",
              "      <td>0.010574487</td>\n",
              "      <td>0.006345963</td>\n",
              "      <td>0.021889961</td>\n",
              "      <td>0</td>\n",
              "      <td>0.111051924</td>\n",
              "      <td>0.026685049</td>\n",
              "      <td>0.021536805</td>\n",
              "      <td>0</td>\n",
              "      <td>0.336278806</td>\n",
              "      <td>0.102604153</td>\n",
              "      <td>0.030742157</td>\n",
              "      <td>0.104350799</td>\n",
              "      <td>0.001971815</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.009789242</td>\n",
              "      <td>0.164854579</td>\n",
              "      <td>0.290821262</td>\n",
              "      <td>2.023672766</td>\n",
              "      <td>0.071828089</td>\n",
              "      <td>0.005717046</td>\n",
              "      <td>0.024120752</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.35548505</td>\n",
              "      <td>0.011347854</td>\n",
              "      <td>0.547136681</td>\n",
              "      <td>0.053415637</td>\n",
              "      <td>0.004346223</td>\n",
              "      <td>0.045614802</td>\n",
              "      <td>0</td>\n",
              "      <td>0.067797517</td>\n",
              "      <td>0</td>\n",
              "      <td>0.030999776</td>\n",
              "      <td>0.106731011</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120 rows × 43 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           pg/mL      pg/mL.1  ...     pg/mL.41     pg/mL.42\n",
              "0              0            0  ...            0            0\n",
              "1       32.88848      0.55984  ...      45.7468     18.96545\n",
              "2       81.61041      0.06288  ...     35.51699     11.14506\n",
              "3       21.05386      0.21039  ...     45.26911      7.55923\n",
              "4       65.32818      0.17781  ...     62.91014     14.23622\n",
              "..           ...          ...  ...          ...          ...\n",
              "115  0.029802322  0.238418579  ...   0.11920929   0.11920929\n",
              "116  0.029802322  0.238418579  ...   0.11920929   0.11920929\n",
              "117   0.00654322  0.090658947  ...  0.038301252  0.120293123\n",
              "118  0.003132865  0.031866869  ...  0.049703948  0.073873469\n",
              "119            0  0.031234313  ...  0.030999776  0.106731011\n",
              "\n",
              "[120 rows x 43 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoKuPh2BiwvF",
        "outputId": "afa05823-a401-4368-bebb-98b52dd59cc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        }
      },
      "source": [
        "df1.to_csv(\"/content/df1.csv\")\n",
        "df1"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1.62496</th>\n",
              "      <th>1.67346</th>\n",
              "      <th>2.30191</th>\n",
              "      <th>1.09831</th>\n",
              "      <th>0.81814</th>\n",
              "      <th>6.64685</th>\n",
              "      <th>4.33614</th>\n",
              "      <th>2.09165</th>\n",
              "      <th>3.05241</th>\n",
              "      <th>2.20853</th>\n",
              "      <th>2.26132</th>\n",
              "      <th>2.50169</th>\n",
              "      <th>0.23786</th>\n",
              "      <th>4.26749</th>\n",
              "      <th>0.64027</th>\n",
              "      <th>3.49041</th>\n",
              "      <th>1.65639</th>\n",
              "      <th>0.18282</th>\n",
              "      <th>-0.00258</th>\n",
              "      <th>4.99097</th>\n",
              "      <th>2.04746</th>\n",
              "      <th>0.91502</th>\n",
              "      <th>3.26891</th>\n",
              "      <th>2.64068</th>\n",
              "      <th>3.40662</th>\n",
              "      <th>1.75302</th>\n",
              "      <th>0.12112</th>\n",
              "      <th>1.8423</th>\n",
              "      <th>2.36297</th>\n",
              "      <th>2.11362</th>\n",
              "      <th>1.17813</th>\n",
              "      <th>3.05439</th>\n",
              "      <th>1.92376</th>\n",
              "      <th>9.62026</th>\n",
              "      <th>8.21851</th>\n",
              "      <th>2.76462</th>\n",
              "      <th>1.76014</th>\n",
              "      <th>2.25881</th>\n",
              "      <th>2.59258</th>\n",
              "      <th>-0.81823</th>\n",
              "      <th>...</th>\n",
              "      <th>0.17781</th>\n",
              "      <th>213.10409</th>\n",
              "      <th>221.91738</th>\n",
              "      <th>3.62822</th>\n",
              "      <th>0.02938</th>\n",
              "      <th>87.22272</th>\n",
              "      <th>8.42985</th>\n",
              "      <th>0.03051</th>\n",
              "      <th>5.64413</th>\n",
              "      <th>0.01686</th>\n",
              "      <th>704.85085</th>\n",
              "      <th>0.01658</th>\n",
              "      <th>170.40823</th>\n",
              "      <th>466.69849</th>\n",
              "      <th>115.24552</th>\n",
              "      <th>4.90308</th>\n",
              "      <th>6.883</th>\n",
              "      <th>0.63191</th>\n",
              "      <th>259.29947</th>\n",
              "      <th>194.00077</th>\n",
              "      <th>774.57014</th>\n",
              "      <th>172.86498</th>\n",
              "      <th>0.52656</th>\n",
              "      <th>0.95139</th>\n",
              "      <th>174.72671</th>\n",
              "      <th>21.80639</th>\n",
              "      <th>13.05225</th>\n",
              "      <th>15.51203</th>\n",
              "      <th>12.51854</th>\n",
              "      <th>358.06817</th>\n",
              "      <th>0.26974</th>\n",
              "      <th>85.96804</th>\n",
              "      <th>502.53571</th>\n",
              "      <th>8.03254</th>\n",
              "      <th>321.30365</th>\n",
              "      <th>609.9247</th>\n",
              "      <th>0.22472</th>\n",
              "      <th>3.80081</th>\n",
              "      <th>138.49611</th>\n",
              "      <th>133.96489</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>idx</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1-026</th>\n",
              "      <td>2.48623</td>\n",
              "      <td>2.42240</td>\n",
              "      <td>2.86282</td>\n",
              "      <td>2.19087</td>\n",
              "      <td>1.65184</td>\n",
              "      <td>5.22605</td>\n",
              "      <td>6.28107</td>\n",
              "      <td>1.36379</td>\n",
              "      <td>3.06955</td>\n",
              "      <td>2.27634</td>\n",
              "      <td>2.07234</td>\n",
              "      <td>2.20169</td>\n",
              "      <td>0.42551</td>\n",
              "      <td>3.32472</td>\n",
              "      <td>1.34123</td>\n",
              "      <td>3.66986</td>\n",
              "      <td>1.94632</td>\n",
              "      <td>0.19312</td>\n",
              "      <td>0.26522</td>\n",
              "      <td>5.33663</td>\n",
              "      <td>1.55030</td>\n",
              "      <td>1.35359</td>\n",
              "      <td>4.92914</td>\n",
              "      <td>1.57654</td>\n",
              "      <td>3.53306</td>\n",
              "      <td>2.05642</td>\n",
              "      <td>0.09155</td>\n",
              "      <td>2.80670</td>\n",
              "      <td>2.72684</td>\n",
              "      <td>2.45478</td>\n",
              "      <td>1.10694</td>\n",
              "      <td>3.07896</td>\n",
              "      <td>1.96822</td>\n",
              "      <td>8.34351</td>\n",
              "      <td>8.50506</td>\n",
              "      <td>2.96392</td>\n",
              "      <td>3.36216</td>\n",
              "      <td>0.08286</td>\n",
              "      <td>1.90096</td>\n",
              "      <td>-0.39492</td>\n",
              "      <td>...</td>\n",
              "      <td>0.20685</td>\n",
              "      <td>303.32904</td>\n",
              "      <td>538.27425</td>\n",
              "      <td>8.17765</td>\n",
              "      <td>0.02086</td>\n",
              "      <td>71.46363</td>\n",
              "      <td>26.32494</td>\n",
              "      <td>0.10661</td>\n",
              "      <td>3.74722</td>\n",
              "      <td>0.01831</td>\n",
              "      <td>671.56909</td>\n",
              "      <td>0.14182</td>\n",
              "      <td>216.81600</td>\n",
              "      <td>758.27687</td>\n",
              "      <td>46.06363</td>\n",
              "      <td>0.43898</td>\n",
              "      <td>10.52915</td>\n",
              "      <td>0.07257</td>\n",
              "      <td>183.78762</td>\n",
              "      <td>133.55088</td>\n",
              "      <td>661.44866</td>\n",
              "      <td>120.35070</td>\n",
              "      <td>0.29943</td>\n",
              "      <td>16.04506</td>\n",
              "      <td>44.84258</td>\n",
              "      <td>91.40858</td>\n",
              "      <td>12.10870</td>\n",
              "      <td>8.32337</td>\n",
              "      <td>7.81655</td>\n",
              "      <td>96.37740</td>\n",
              "      <td>0.09573</td>\n",
              "      <td>59.29720</td>\n",
              "      <td>486.41575</td>\n",
              "      <td>12.10414</td>\n",
              "      <td>520.15895</td>\n",
              "      <td>498.29936</td>\n",
              "      <td>1.25437</td>\n",
              "      <td>14.47713</td>\n",
              "      <td>122.28971</td>\n",
              "      <td>202.38365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5-160</th>\n",
              "      <td>2.51644</td>\n",
              "      <td>1.42410</td>\n",
              "      <td>2.66267</td>\n",
              "      <td>1.01372</td>\n",
              "      <td>1.29349</td>\n",
              "      <td>6.88173</td>\n",
              "      <td>5.36133</td>\n",
              "      <td>1.76990</td>\n",
              "      <td>2.91154</td>\n",
              "      <td>1.49769</td>\n",
              "      <td>2.99616</td>\n",
              "      <td>2.28051</td>\n",
              "      <td>0.48042</td>\n",
              "      <td>3.31877</td>\n",
              "      <td>1.08041</td>\n",
              "      <td>3.35323</td>\n",
              "      <td>2.07311</td>\n",
              "      <td>-0.36846</td>\n",
              "      <td>1.32608</td>\n",
              "      <td>4.59227</td>\n",
              "      <td>1.58213</td>\n",
              "      <td>0.95043</td>\n",
              "      <td>4.03325</td>\n",
              "      <td>1.52062</td>\n",
              "      <td>2.37259</td>\n",
              "      <td>2.10056</td>\n",
              "      <td>0.64504</td>\n",
              "      <td>2.14644</td>\n",
              "      <td>2.11373</td>\n",
              "      <td>3.16644</td>\n",
              "      <td>1.47408</td>\n",
              "      <td>3.79823</td>\n",
              "      <td>2.95064</td>\n",
              "      <td>7.91852</td>\n",
              "      <td>8.24384</td>\n",
              "      <td>3.23319</td>\n",
              "      <td>2.34342</td>\n",
              "      <td>0.09271</td>\n",
              "      <td>2.51827</td>\n",
              "      <td>-0.24256</td>\n",
              "      <td>...</td>\n",
              "      <td>0.10408</td>\n",
              "      <td>358.65048</td>\n",
              "      <td>340.85689</td>\n",
              "      <td>6.99217</td>\n",
              "      <td>0.01757</td>\n",
              "      <td>93.01169</td>\n",
              "      <td>13.04904</td>\n",
              "      <td>0.08250</td>\n",
              "      <td>4.61720</td>\n",
              "      <td>0.03077</td>\n",
              "      <td>1068.27309</td>\n",
              "      <td>0.03470</td>\n",
              "      <td>154.59001</td>\n",
              "      <td>617.55142</td>\n",
              "      <td>170.40326</td>\n",
              "      <td>0.52644</td>\n",
              "      <td>10.59590</td>\n",
              "      <td>0.30125</td>\n",
              "      <td>317.73481</td>\n",
              "      <td>294.00150</td>\n",
              "      <td>812.09222</td>\n",
              "      <td>241.02610</td>\n",
              "      <td>0.08465</td>\n",
              "      <td>1.11067</td>\n",
              "      <td>134.06110</td>\n",
              "      <td>22.78650</td>\n",
              "      <td>18.97588</td>\n",
              "      <td>25.67310</td>\n",
              "      <td>23.13243</td>\n",
              "      <td>293.96144</td>\n",
              "      <td>0.15127</td>\n",
              "      <td>68.91151</td>\n",
              "      <td>774.71499</td>\n",
              "      <td>7.07773</td>\n",
              "      <td>254.5813</td>\n",
              "      <td>789.30173</td>\n",
              "      <td>0.09381</td>\n",
              "      <td>4.41604</td>\n",
              "      <td>139.86582</td>\n",
              "      <td>237.97159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1-057</th>\n",
              "      <td>2.18885</td>\n",
              "      <td>1.92321</td>\n",
              "      <td>2.39325</td>\n",
              "      <td>0.88833</td>\n",
              "      <td>1.09321</td>\n",
              "      <td>6.04054</td>\n",
              "      <td>5.61824</td>\n",
              "      <td>1.48480</td>\n",
              "      <td>2.63778</td>\n",
              "      <td>1.89899</td>\n",
              "      <td>2.24617</td>\n",
              "      <td>2.51717</td>\n",
              "      <td>0.22299</td>\n",
              "      <td>3.55706</td>\n",
              "      <td>1.45571</td>\n",
              "      <td>3.38502</td>\n",
              "      <td>1.01922</td>\n",
              "      <td>-0.12618</td>\n",
              "      <td>-0.16629</td>\n",
              "      <td>4.67541</td>\n",
              "      <td>1.99160</td>\n",
              "      <td>1.23945</td>\n",
              "      <td>3.79489</td>\n",
              "      <td>1.43101</td>\n",
              "      <td>3.61266</td>\n",
              "      <td>1.66163</td>\n",
              "      <td>-0.16582</td>\n",
              "      <td>2.32807</td>\n",
              "      <td>2.26160</td>\n",
              "      <td>2.96670</td>\n",
              "      <td>2.96144</td>\n",
              "      <td>2.31760</td>\n",
              "      <td>1.77681</td>\n",
              "      <td>7.87806</td>\n",
              "      <td>7.78202</td>\n",
              "      <td>2.28510</td>\n",
              "      <td>2.74136</td>\n",
              "      <td>0.19890</td>\n",
              "      <td>1.83341</td>\n",
              "      <td>-0.65013</td>\n",
              "      <td>...</td>\n",
              "      <td>0.12938</td>\n",
              "      <td>253.87535</td>\n",
              "      <td>421.66801</td>\n",
              "      <td>2.64339</td>\n",
              "      <td>0.01609</td>\n",
              "      <td>44.71970</td>\n",
              "      <td>18.09288</td>\n",
              "      <td>0.13899</td>\n",
              "      <td>5.61282</td>\n",
              "      <td>0.03867</td>\n",
              "      <td>919.53078</td>\n",
              "      <td>0.10944</td>\n",
              "      <td>114.11861</td>\n",
              "      <td>651.62065</td>\n",
              "      <td>111.86342</td>\n",
              "      <td>1.35037</td>\n",
              "      <td>6.24202</td>\n",
              "      <td>0.68117</td>\n",
              "      <td>176.17564</td>\n",
              "      <td>175.41600</td>\n",
              "      <td>565.65771</td>\n",
              "      <td>160.38016</td>\n",
              "      <td>0.28778</td>\n",
              "      <td>1.32927</td>\n",
              "      <td>125.85381</td>\n",
              "      <td>20.58934</td>\n",
              "      <td>15.22033</td>\n",
              "      <td>6.56485</td>\n",
              "      <td>5.91469</td>\n",
              "      <td>121.55361</td>\n",
              "      <td>0.65329</td>\n",
              "      <td>71.92847</td>\n",
              "      <td>1002.93908</td>\n",
              "      <td>16.51496</td>\n",
              "      <td>855.03256</td>\n",
              "      <td>354.65773</td>\n",
              "      <td>0.88035</td>\n",
              "      <td>9.17426</td>\n",
              "      <td>125.66622</td>\n",
              "      <td>108.02697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2-026</th>\n",
              "      <td>0.95533</td>\n",
              "      <td>1.76364</td>\n",
              "      <td>1.51776</td>\n",
              "      <td>0.58076</td>\n",
              "      <td>0.69636</td>\n",
              "      <td>5.90082</td>\n",
              "      <td>2.73350</td>\n",
              "      <td>2.19529</td>\n",
              "      <td>3.23191</td>\n",
              "      <td>0.98465</td>\n",
              "      <td>3.04368</td>\n",
              "      <td>1.81168</td>\n",
              "      <td>0.40645</td>\n",
              "      <td>4.67359</td>\n",
              "      <td>0.77899</td>\n",
              "      <td>3.54338</td>\n",
              "      <td>0.86176</td>\n",
              "      <td>-0.30449</td>\n",
              "      <td>0.33032</td>\n",
              "      <td>3.76795</td>\n",
              "      <td>1.80045</td>\n",
              "      <td>1.22304</td>\n",
              "      <td>2.54004</td>\n",
              "      <td>1.40856</td>\n",
              "      <td>2.94573</td>\n",
              "      <td>1.82917</td>\n",
              "      <td>-0.04098</td>\n",
              "      <td>1.54045</td>\n",
              "      <td>1.94063</td>\n",
              "      <td>1.98781</td>\n",
              "      <td>1.19970</td>\n",
              "      <td>2.33297</td>\n",
              "      <td>2.39776</td>\n",
              "      <td>8.11420</td>\n",
              "      <td>7.38280</td>\n",
              "      <td>3.21477</td>\n",
              "      <td>1.33949</td>\n",
              "      <td>0.16812</td>\n",
              "      <td>2.10073</td>\n",
              "      <td>-0.09575</td>\n",
              "      <td>...</td>\n",
              "      <td>0.08390</td>\n",
              "      <td>288.98831</td>\n",
              "      <td>179.43192</td>\n",
              "      <td>3.47052</td>\n",
              "      <td>0.03396</td>\n",
              "      <td>54.19521</td>\n",
              "      <td>17.44564</td>\n",
              "      <td>0.05448</td>\n",
              "      <td>2.32773</td>\n",
              "      <td>0.05600</td>\n",
              "      <td>628.13830</td>\n",
              "      <td>1.58528</td>\n",
              "      <td>78.84977</td>\n",
              "      <td>461.00918</td>\n",
              "      <td>140.69730</td>\n",
              "      <td>0.51683</td>\n",
              "      <td>6.11277</td>\n",
              "      <td>1.46852</td>\n",
              "      <td>184.06353</td>\n",
              "      <td>84.07957</td>\n",
              "      <td>225.18583</td>\n",
              "      <td>144.84396</td>\n",
              "      <td>0.24226</td>\n",
              "      <td>0.70483</td>\n",
              "      <td>174.41203</td>\n",
              "      <td>17.54087</td>\n",
              "      <td>10.35005</td>\n",
              "      <td>4.45898</td>\n",
              "      <td>11.75630</td>\n",
              "      <td>133.80343</td>\n",
              "      <td>0.15385</td>\n",
              "      <td>37.76285</td>\n",
              "      <td>719.29010</td>\n",
              "      <td>6.70568</td>\n",
              "      <td>541.45392</td>\n",
              "      <td>406.54303</td>\n",
              "      <td>0.06454</td>\n",
              "      <td>6.68895</td>\n",
              "      <td>162.27642</td>\n",
              "      <td>90.57557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1-098</th>\n",
              "      <td>2.89161</td>\n",
              "      <td>1.11551</td>\n",
              "      <td>3.99761</td>\n",
              "      <td>1.85142</td>\n",
              "      <td>1.87377</td>\n",
              "      <td>5.69155</td>\n",
              "      <td>5.97976</td>\n",
              "      <td>1.76262</td>\n",
              "      <td>2.96242</td>\n",
              "      <td>2.02689</td>\n",
              "      <td>2.34766</td>\n",
              "      <td>3.04282</td>\n",
              "      <td>0.86051</td>\n",
              "      <td>5.42346</td>\n",
              "      <td>1.99981</td>\n",
              "      <td>3.53259</td>\n",
              "      <td>2.11203</td>\n",
              "      <td>0.17115</td>\n",
              "      <td>0.56939</td>\n",
              "      <td>5.09035</td>\n",
              "      <td>2.27526</td>\n",
              "      <td>1.96526</td>\n",
              "      <td>4.92574</td>\n",
              "      <td>1.84684</td>\n",
              "      <td>2.97131</td>\n",
              "      <td>2.68184</td>\n",
              "      <td>0.65976</td>\n",
              "      <td>2.99761</td>\n",
              "      <td>2.34217</td>\n",
              "      <td>2.96187</td>\n",
              "      <td>0.72550</td>\n",
              "      <td>2.73521</td>\n",
              "      <td>2.62436</td>\n",
              "      <td>8.12573</td>\n",
              "      <td>8.07679</td>\n",
              "      <td>3.35168</td>\n",
              "      <td>3.01710</td>\n",
              "      <td>1.26085</td>\n",
              "      <td>1.82416</td>\n",
              "      <td>1.40533</td>\n",
              "      <td>...</td>\n",
              "      <td>0.41453</td>\n",
              "      <td>435.77899</td>\n",
              "      <td>436.55285</td>\n",
              "      <td>4.33455</td>\n",
              "      <td>0.01037</td>\n",
              "      <td>48.68184</td>\n",
              "      <td>29.89121</td>\n",
              "      <td>0.17327</td>\n",
              "      <td>8.39665</td>\n",
              "      <td>0.02531</td>\n",
              "      <td>812.44530</td>\n",
              "      <td>0.24094</td>\n",
              "      <td>134.35920</td>\n",
              "      <td>863.40983</td>\n",
              "      <td>66.57326</td>\n",
              "      <td>1.59393</td>\n",
              "      <td>6.81663</td>\n",
              "      <td>1.85431</td>\n",
              "      <td>515.64512</td>\n",
              "      <td>183.41078</td>\n",
              "      <td>702.72169</td>\n",
              "      <td>101.18288</td>\n",
              "      <td>0.32068</td>\n",
              "      <td>1.36648</td>\n",
              "      <td>136.72039</td>\n",
              "      <td>21.09916</td>\n",
              "      <td>14.29151</td>\n",
              "      <td>7.12491</td>\n",
              "      <td>7.09622</td>\n",
              "      <td>295.78964</td>\n",
              "      <td>0.32768</td>\n",
              "      <td>100.08473</td>\n",
              "      <td>383.96980</td>\n",
              "      <td>14.27227</td>\n",
              "      <td>617.10761</td>\n",
              "      <td>395.25388</td>\n",
              "      <td>0.54780</td>\n",
              "      <td>17.67580</td>\n",
              "      <td>140.29749</td>\n",
              "      <td>150.26855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2-088</th>\n",
              "      <td>1.61035</td>\n",
              "      <td>0.89827</td>\n",
              "      <td>2.73902</td>\n",
              "      <td>0.41534</td>\n",
              "      <td>0.58517</td>\n",
              "      <td>7.03324</td>\n",
              "      <td>3.84065</td>\n",
              "      <td>2.49330</td>\n",
              "      <td>2.81214</td>\n",
              "      <td>0.76543</td>\n",
              "      <td>2.48453</td>\n",
              "      <td>3.04594</td>\n",
              "      <td>0.19381</td>\n",
              "      <td>3.54850</td>\n",
              "      <td>0.89292</td>\n",
              "      <td>3.36624</td>\n",
              "      <td>1.06472</td>\n",
              "      <td>-0.71524</td>\n",
              "      <td>-0.03761</td>\n",
              "      <td>3.89446</td>\n",
              "      <td>1.22798</td>\n",
              "      <td>1.10823</td>\n",
              "      <td>3.69572</td>\n",
              "      <td>1.54578</td>\n",
              "      <td>4.00890</td>\n",
              "      <td>2.13415</td>\n",
              "      <td>-0.17007</td>\n",
              "      <td>1.83491</td>\n",
              "      <td>1.92457</td>\n",
              "      <td>2.49502</td>\n",
              "      <td>0.78495</td>\n",
              "      <td>2.53069</td>\n",
              "      <td>2.02063</td>\n",
              "      <td>8.33469</td>\n",
              "      <td>8.25276</td>\n",
              "      <td>2.75813</td>\n",
              "      <td>1.64108</td>\n",
              "      <td>-0.20859</td>\n",
              "      <td>1.99688</td>\n",
              "      <td>-1.11468</td>\n",
              "      <td>...</td>\n",
              "      <td>0.04090</td>\n",
              "      <td>212.73539</td>\n",
              "      <td>104.24874</td>\n",
              "      <td>6.31880</td>\n",
              "      <td>0.00462</td>\n",
              "      <td>46.80731</td>\n",
              "      <td>15.31681</td>\n",
              "      <td>0.08264</td>\n",
              "      <td>6.26987</td>\n",
              "      <td>0.01432</td>\n",
              "      <td>606.45082</td>\n",
              "      <td>0.09264</td>\n",
              "      <td>139.20966</td>\n",
              "      <td>550.45007</td>\n",
              "      <td>140.95289</td>\n",
              "      <td>9.31415</td>\n",
              "      <td>5.18196</td>\n",
              "      <td>0.41235</td>\n",
              "      <td>362.20526</td>\n",
              "      <td>161.89776</td>\n",
              "      <td>499.16118</td>\n",
              "      <td>81.72507</td>\n",
              "      <td>0.57897</td>\n",
              "      <td>6.16326</td>\n",
              "      <td>103.99597</td>\n",
              "      <td>14.14918</td>\n",
              "      <td>19.37506</td>\n",
              "      <td>4.28038</td>\n",
              "      <td>9.81332</td>\n",
              "      <td>201.83116</td>\n",
              "      <td>0.14074</td>\n",
              "      <td>107.12261</td>\n",
              "      <td>645.16464</td>\n",
              "      <td>5.04863</td>\n",
              "      <td>172.61868</td>\n",
              "      <td>261.72918</td>\n",
              "      <td>0.11615</td>\n",
              "      <td>3.63674</td>\n",
              "      <td>115.92902</td>\n",
              "      <td>108.37137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5-096</th>\n",
              "      <td>2.74508</td>\n",
              "      <td>2.11575</td>\n",
              "      <td>2.52443</td>\n",
              "      <td>1.17342</td>\n",
              "      <td>1.72132</td>\n",
              "      <td>6.72035</td>\n",
              "      <td>5.55985</td>\n",
              "      <td>1.27579</td>\n",
              "      <td>3.11321</td>\n",
              "      <td>1.40379</td>\n",
              "      <td>3.17532</td>\n",
              "      <td>2.21058</td>\n",
              "      <td>0.46948</td>\n",
              "      <td>4.53919</td>\n",
              "      <td>1.40651</td>\n",
              "      <td>3.26059</td>\n",
              "      <td>1.69254</td>\n",
              "      <td>-0.18422</td>\n",
              "      <td>0.04472</td>\n",
              "      <td>4.77000</td>\n",
              "      <td>1.76125</td>\n",
              "      <td>1.49812</td>\n",
              "      <td>3.87958</td>\n",
              "      <td>2.05518</td>\n",
              "      <td>3.38805</td>\n",
              "      <td>2.37046</td>\n",
              "      <td>-0.45803</td>\n",
              "      <td>2.54854</td>\n",
              "      <td>2.23869</td>\n",
              "      <td>2.71556</td>\n",
              "      <td>0.76992</td>\n",
              "      <td>3.01852</td>\n",
              "      <td>2.41117</td>\n",
              "      <td>8.17567</td>\n",
              "      <td>8.14764</td>\n",
              "      <td>2.66762</td>\n",
              "      <td>2.67858</td>\n",
              "      <td>0.17502</td>\n",
              "      <td>2.65150</td>\n",
              "      <td>-1.27876</td>\n",
              "      <td>...</td>\n",
              "      <td>0.02654</td>\n",
              "      <td>316.86857</td>\n",
              "      <td>464.24718</td>\n",
              "      <td>3.69271</td>\n",
              "      <td>0.00359</td>\n",
              "      <td>55.82031</td>\n",
              "      <td>28.91780</td>\n",
              "      <td>0.07587</td>\n",
              "      <td>3.13398</td>\n",
              "      <td>0.02021</td>\n",
              "      <td>784.72983</td>\n",
              "      <td>0.14615</td>\n",
              "      <td>155.09979</td>\n",
              "      <td>851.78968</td>\n",
              "      <td>170.05552</td>\n",
              "      <td>0.08259</td>\n",
              "      <td>9.39931</td>\n",
              "      <td>0.56454</td>\n",
              "      <td>285.82363</td>\n",
              "      <td>190.80872</td>\n",
              "      <td>551.74885</td>\n",
              "      <td>86.78308</td>\n",
              "      <td>0.16722</td>\n",
              "      <td>1.48932</td>\n",
              "      <td>171.59196</td>\n",
              "      <td>21.55293</td>\n",
              "      <td>11.54325</td>\n",
              "      <td>7.47431</td>\n",
              "      <td>19.41750</td>\n",
              "      <td>222.93783</td>\n",
              "      <td>0.44936</td>\n",
              "      <td>86.86604</td>\n",
              "      <td>475.91341</td>\n",
              "      <td>23.23763</td>\n",
              "      <td>283.319</td>\n",
              "      <td>586.27848</td>\n",
              "      <td>0.25796</td>\n",
              "      <td>11.50231</td>\n",
              "      <td>146.51925</td>\n",
              "      <td>145.39921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5-089</th>\n",
              "      <td>2.08981</td>\n",
              "      <td>1.43077</td>\n",
              "      <td>3.78957</td>\n",
              "      <td>1.52396</td>\n",
              "      <td>1.41249</td>\n",
              "      <td>6.70450</td>\n",
              "      <td>4.48548</td>\n",
              "      <td>2.03393</td>\n",
              "      <td>2.53809</td>\n",
              "      <td>1.05581</td>\n",
              "      <td>3.44247</td>\n",
              "      <td>1.30474</td>\n",
              "      <td>0.45290</td>\n",
              "      <td>3.74032</td>\n",
              "      <td>1.18643</td>\n",
              "      <td>3.84345</td>\n",
              "      <td>1.34704</td>\n",
              "      <td>0.22536</td>\n",
              "      <td>0.05981</td>\n",
              "      <td>3.62223</td>\n",
              "      <td>1.80530</td>\n",
              "      <td>1.41704</td>\n",
              "      <td>2.88417</td>\n",
              "      <td>1.78812</td>\n",
              "      <td>3.43252</td>\n",
              "      <td>2.25947</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>2.23018</td>\n",
              "      <td>1.79770</td>\n",
              "      <td>2.88677</td>\n",
              "      <td>2.30834</td>\n",
              "      <td>2.80592</td>\n",
              "      <td>2.51490</td>\n",
              "      <td>7.82519</td>\n",
              "      <td>8.52809</td>\n",
              "      <td>3.73084</td>\n",
              "      <td>1.97028</td>\n",
              "      <td>0.24928</td>\n",
              "      <td>2.75521</td>\n",
              "      <td>-0.44774</td>\n",
              "      <td>...</td>\n",
              "      <td>0.01977</td>\n",
              "      <td>312.66247</td>\n",
              "      <td>164.26069</td>\n",
              "      <td>2.53053</td>\n",
              "      <td>0.00198</td>\n",
              "      <td>43.05317</td>\n",
              "      <td>14.84465</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.10994</td>\n",
              "      <td>0.01922</td>\n",
              "      <td>669.65398</td>\n",
              "      <td>0.02587</td>\n",
              "      <td>181.25387</td>\n",
              "      <td>555.32129</td>\n",
              "      <td>85.12124</td>\n",
              "      <td>0.24785</td>\n",
              "      <td>6.61516</td>\n",
              "      <td>0.30295</td>\n",
              "      <td>226.71323</td>\n",
              "      <td>222.11226</td>\n",
              "      <td>484.45214</td>\n",
              "      <td>98.48073</td>\n",
              "      <td>0.10164</td>\n",
              "      <td>0.65080</td>\n",
              "      <td>93.53124</td>\n",
              "      <td>14.72271</td>\n",
              "      <td>9.95253</td>\n",
              "      <td>4.98294</td>\n",
              "      <td>9.63247</td>\n",
              "      <td>174.10991</td>\n",
              "      <td>0.07567</td>\n",
              "      <td>58.64828</td>\n",
              "      <td>215.77872</td>\n",
              "      <td>12.66905</td>\n",
              "      <td>217.34673</td>\n",
              "      <td>544.59983</td>\n",
              "      <td>0.43952</td>\n",
              "      <td>5.21303</td>\n",
              "      <td>105.56371</td>\n",
              "      <td>143.84551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2-015</th>\n",
              "      <td>1.54849</td>\n",
              "      <td>2.37265</td>\n",
              "      <td>1.94544</td>\n",
              "      <td>0.33856</td>\n",
              "      <td>0.62158</td>\n",
              "      <td>6.42383</td>\n",
              "      <td>4.42008</td>\n",
              "      <td>1.95705</td>\n",
              "      <td>2.86389</td>\n",
              "      <td>1.25292</td>\n",
              "      <td>3.81952</td>\n",
              "      <td>1.32452</td>\n",
              "      <td>0.46196</td>\n",
              "      <td>4.86208</td>\n",
              "      <td>0.88328</td>\n",
              "      <td>3.68536</td>\n",
              "      <td>1.19192</td>\n",
              "      <td>-0.57990</td>\n",
              "      <td>-0.00932</td>\n",
              "      <td>4.57350</td>\n",
              "      <td>1.62156</td>\n",
              "      <td>1.04163</td>\n",
              "      <td>3.12440</td>\n",
              "      <td>2.29579</td>\n",
              "      <td>3.22493</td>\n",
              "      <td>1.28871</td>\n",
              "      <td>0.00123</td>\n",
              "      <td>1.51981</td>\n",
              "      <td>2.07370</td>\n",
              "      <td>2.25940</td>\n",
              "      <td>1.29434</td>\n",
              "      <td>3.28029</td>\n",
              "      <td>2.68214</td>\n",
              "      <td>7.96000</td>\n",
              "      <td>8.32343</td>\n",
              "      <td>3.23627</td>\n",
              "      <td>1.61376</td>\n",
              "      <td>0.64341</td>\n",
              "      <td>2.45568</td>\n",
              "      <td>0.53752</td>\n",
              "      <td>...</td>\n",
              "      <td>0.09163</td>\n",
              "      <td>364.44988</td>\n",
              "      <td>226.49315</td>\n",
              "      <td>5.37359</td>\n",
              "      <td>0.01126</td>\n",
              "      <td>64.74724</td>\n",
              "      <td>10.41876</td>\n",
              "      <td>0.03102</td>\n",
              "      <td>1.18536</td>\n",
              "      <td>0.08616</td>\n",
              "      <td>990.56097</td>\n",
              "      <td>0.04336</td>\n",
              "      <td>186.35687</td>\n",
              "      <td>484.24862</td>\n",
              "      <td>84.05331</td>\n",
              "      <td>0.72593</td>\n",
              "      <td>6.44323</td>\n",
              "      <td>0.17058</td>\n",
              "      <td>328.07296</td>\n",
              "      <td>200.77648</td>\n",
              "      <td>688.02978</td>\n",
              "      <td>131.71582</td>\n",
              "      <td>0.39785</td>\n",
              "      <td>26.41284</td>\n",
              "      <td>126.37865</td>\n",
              "      <td>20.61250</td>\n",
              "      <td>10.29773</td>\n",
              "      <td>6.78130</td>\n",
              "      <td>10.17965</td>\n",
              "      <td>199.34320</td>\n",
              "      <td>0.21705</td>\n",
              "      <td>59.15150</td>\n",
              "      <td>305.40885</td>\n",
              "      <td>14.95930</td>\n",
              "      <td>551.17072</td>\n",
              "      <td>448.83428</td>\n",
              "      <td>0.54213</td>\n",
              "      <td>3.22558</td>\n",
              "      <td>100.71141</td>\n",
              "      <td>167.31158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1-073</th>\n",
              "      <td>1.40763</td>\n",
              "      <td>2.32194</td>\n",
              "      <td>2.53494</td>\n",
              "      <td>0.55014</td>\n",
              "      <td>1.07079</td>\n",
              "      <td>6.99025</td>\n",
              "      <td>3.66657</td>\n",
              "      <td>1.47058</td>\n",
              "      <td>3.56960</td>\n",
              "      <td>1.36074</td>\n",
              "      <td>2.10632</td>\n",
              "      <td>3.84408</td>\n",
              "      <td>0.28915</td>\n",
              "      <td>3.95176</td>\n",
              "      <td>1.06300</td>\n",
              "      <td>3.73548</td>\n",
              "      <td>1.09867</td>\n",
              "      <td>-0.56262</td>\n",
              "      <td>0.29702</td>\n",
              "      <td>4.36775</td>\n",
              "      <td>1.62465</td>\n",
              "      <td>1.02897</td>\n",
              "      <td>2.65905</td>\n",
              "      <td>1.17706</td>\n",
              "      <td>3.49666</td>\n",
              "      <td>2.48784</td>\n",
              "      <td>0.10890</td>\n",
              "      <td>1.89652</td>\n",
              "      <td>2.18250</td>\n",
              "      <td>3.39721</td>\n",
              "      <td>0.76407</td>\n",
              "      <td>3.74858</td>\n",
              "      <td>2.14870</td>\n",
              "      <td>9.14687</td>\n",
              "      <td>8.56458</td>\n",
              "      <td>3.02723</td>\n",
              "      <td>1.55649</td>\n",
              "      <td>-0.13517</td>\n",
              "      <td>2.78867</td>\n",
              "      <td>-0.72367</td>\n",
              "      <td>...</td>\n",
              "      <td>0.15968</td>\n",
              "      <td>282.28874</td>\n",
              "      <td>198.85158</td>\n",
              "      <td>9.41758</td>\n",
              "      <td>0.00334</td>\n",
              "      <td>146.12713</td>\n",
              "      <td>17.96304</td>\n",
              "      <td>0.14800</td>\n",
              "      <td>12.44588</td>\n",
              "      <td>0.01741</td>\n",
              "      <td>600.15844</td>\n",
              "      <td>0.06612</td>\n",
              "      <td>170.21377</td>\n",
              "      <td>758.78519</td>\n",
              "      <td>129.83505</td>\n",
              "      <td>1.27187</td>\n",
              "      <td>6.19870</td>\n",
              "      <td>1.60705</td>\n",
              "      <td>448.88050</td>\n",
              "      <td>238.99948</td>\n",
              "      <td>812.17425</td>\n",
              "      <td>204.58520</td>\n",
              "      <td>0.96732</td>\n",
              "      <td>0.97148</td>\n",
              "      <td>326.38609</td>\n",
              "      <td>21.97049</td>\n",
              "      <td>11.45213</td>\n",
              "      <td>11.42666</td>\n",
              "      <td>41.31382</td>\n",
              "      <td>530.68688</td>\n",
              "      <td>0.23515</td>\n",
              "      <td>116.73231</td>\n",
              "      <td>2175.58531</td>\n",
              "      <td>10.09897</td>\n",
              "      <td>211.4458</td>\n",
              "      <td>676.46110</td>\n",
              "      <td>1.06761</td>\n",
              "      <td>11.31238</td>\n",
              "      <td>167.02124</td>\n",
              "      <td>227.47854</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>90 rows × 225 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       1.62496  1.67346  2.30191  ...   3.80081  138.49611  133.96489\n",
              "idx                               ...                                \n",
              "1-026  2.48623  2.42240  2.86282  ...  14.47713  122.28971  202.38365\n",
              "5-160  2.51644  1.42410  2.66267  ...   4.41604  139.86582  237.97159\n",
              "1-057  2.18885  1.92321  2.39325  ...   9.17426  125.66622  108.02697\n",
              "2-026  0.95533  1.76364  1.51776  ...   6.68895  162.27642   90.57557\n",
              "1-098  2.89161  1.11551  3.99761  ...  17.67580  140.29749  150.26855\n",
              "...        ...      ...      ...  ...       ...        ...        ...\n",
              "2-088  1.61035  0.89827  2.73902  ...   3.63674  115.92902  108.37137\n",
              "5-096  2.74508  2.11575  2.52443  ...  11.50231  146.51925  145.39921\n",
              "5-089  2.08981  1.43077  3.78957  ...   5.21303  105.56371  143.84551\n",
              "2-015  1.54849  2.37265  1.94544  ...   3.22558  100.71141  167.31158\n",
              "1-073  1.40763  2.32194  2.53494  ...  11.31238  167.02124  227.47854\n",
              "\n",
              "[90 rows x 225 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIyazhMufXf2",
        "outputId": "fe06682c-3448-4913-f75e-0f22049065dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "scaler.fit(X)\n",
        "#print(normalized_arr)\n",
        "#scaler"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-1bbd8f620548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#print(normalized_arr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#scaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    371\u001b[0m         X = check_array(X,\n\u001b[1;32m    372\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                         force_all_finite=\"allow-nan\")\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mdata_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '> ULOQ'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHfaxGhy05SD",
        "outputId": "8c4dd3e2-637f-4968-83b9-d5020675d9f9"
      },
      "source": [
        "pip install pyscm-ml   ### Ici on installe pyscm-ml librairie indispensable pour rouler l'algorithme RandomSCM"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyscm-ml in /usr/local/lib/python3.7/dist-packages (1.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyscm-ml) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pyscm-ml) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyscm-ml) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyscm-ml) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyscm-ml) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GjhW8n00Fdv"
      },
      "source": [
        "### Ici c'est l'algorithme SCM avec ses classes et toutes ses fonctions\n",
        "from sklearn.base import ClassifierMixin\n",
        "from sklearn.ensemble import BaseEnsemble\n",
        "from pyscm import SetCoveringMachineClassifier\n",
        "from pyscm.rules import DecisionStump\n",
        "\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted, check_random_state\n",
        "from sklearn.utils.random import sample_without_replacement\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numbers\n",
        "import itertools\n",
        "import numpy as np\n",
        "from warnings import warn\n",
        "from joblib import effective_n_jobs, Parallel, delayed\n",
        "\n",
        "MAX_INT = np.iinfo(np.int32).max\n",
        "def _parallel_build_estimators(idx, ensemble, p_of_estims, seeds, X, y, tiebreaker):\n",
        "    \"\"\"\n",
        "    Fit SCM estimators on subsamples of the training data\n",
        "    \"\"\"\n",
        "    estimators = []\n",
        "    estim_features = []\n",
        "    for k in idx:\n",
        "        p_param = p_of_estims[k] # p param for the classifier to fit\n",
        "        random_state = seeds[k]\n",
        "        estim = SetCoveringMachineClassifier(p=p_param, max_rules=ensemble.max_rules, model_type=ensemble.model_type, random_state=random_state)\n",
        "        feature_indices = sample_without_replacement(ensemble._pop_features, ensemble._max_features, random_state=random_state)\n",
        "        samples_indices = sample_without_replacement(ensemble._pop_samples, ensemble._max_samples, random_state=random_state)\n",
        "        Xk = (X[samples_indices])[:, feature_indices]\n",
        "        yk = y[samples_indices]\n",
        "        if len(list(set(yk))) < 2:\n",
        "            raise ValueError(\"One of the subsamples contains elements from only 1 class, try increase max_samples value\")\n",
        "        if tiebreaker is None:\n",
        "            estim.fit(Xk, yk)\n",
        "        else:\n",
        "            estim.fit(Xk, yk, tiebreaker)\n",
        "        estim_features.append(feature_indices)\n",
        "        estimators.append(estim)\n",
        "    return estimators, estim_features\n",
        "\n",
        "\n",
        "def _parallel_predict_proba(ensemble, X, idx, results):\n",
        "    \"\"\"\n",
        "    Compute predictions of SCM estimators\n",
        "    \"\"\"\n",
        "    for k in idx:\n",
        "        res = ensemble.estimators[k].predict(X[:, ensemble.estim_features[k]])\n",
        "        if ensemble.min_cq_combination:\n",
        "            res[res==0] = -1\n",
        "            results = results + ensemble.min_cq_weights[k]*res\n",
        "        else:\n",
        "            results = results + res\n",
        "    return results\n",
        "\n",
        "\n",
        "def _partition_estimators(n_estimators, n_jobs):\n",
        "    \"\"\"Private function used to partition estimators between jobs.\"\"\"\n",
        "    # Compute the number of jobs\n",
        "    n_jobs = min(effective_n_jobs(n_jobs), n_estimators)\n",
        "\n",
        "    # Partition estimators between jobs\n",
        "    n_estimators_per_job = np.full(n_jobs, n_estimators // n_jobs,\n",
        "                                   dtype=np.int)\n",
        "    n_estimators_per_job[:n_estimators % n_jobs] += 1\n",
        "    starts = np.cumsum(n_estimators_per_job)\n",
        "\n",
        "    return n_jobs, n_estimators_per_job.tolist(), [0] + starts.tolist()\n",
        "\n",
        "\n",
        "class RandomScmClassifier(BaseEnsemble, ClassifierMixin):\n",
        "    \"\"\"A Bagging classifier for SetCoveringMachineClassifier()\n",
        "    The base estimators are built on subsets of both samples\n",
        "    and features.\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_estimators : int, default=10\n",
        "        The number of base estimators in the ensemble.\n",
        "    max_samples : int or float, default=1.0\n",
        "        The number of samples to draw from X to train each base estimator with\n",
        "        replacement.\n",
        "        - If int, then draw `max_samples` samples.\n",
        "        - If float, then draw `max_samples * X.shape[0]` samples.\n",
        "    max_features : int or float, default=1.0\n",
        "        The number of features to draw from X to train each base estimator (\n",
        "        without replacement.\n",
        "        - If int, then draw `max_features` features.\n",
        "        - If float, then draw `max_features * X.shape[1]` features.\n",
        "    max_rules : int\n",
        "        maximal number of rules for the scm estimators\n",
        "    p_options : list of float with len =< n_estimators, default=[1.0]\n",
        "        The estimators will be fitted with values of p found in p_options\n",
        "        let k be k = n_estimators/len(p_options),\n",
        "        the k first estimators will have p=p_options[0],\n",
        "        the next k estimators will have p=p_options[1] and so on...\n",
        "    model_type : string, default='conjunction'\n",
        "        type of estimators to build\n",
        "        accepted values : 'conjunction', 'disjunction'\n",
        "    n_jobs : int, default=None\n",
        "        The number of jobs to run in parallel for both fit and\n",
        "        predict. 'None' means 1. '-1' means using all\n",
        "        processors.\n",
        "    random_state : int or RandomState, default=None\n",
        "        Controls the random resampling of the original dataset\n",
        "        (sample wise and feature wise).\n",
        "        If the base estimator accepts a `random_state` attribute, a different\n",
        "        seed is generated for each instance in the ensemble.\n",
        "        Pass an int for reproducible output across multiple function calls.\n",
        "        See :term:`Glossary <random_state>`.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    n_features_ : int\n",
        "        The number of features when :meth:`fit` is performed.\n",
        "    estimators_ : list of estimators\n",
        "        The collection of fitted base estimators.\n",
        "    estim_features : list of arrays\n",
        "        The subset of drawn features for each base estimator.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> random_scm = RandomScmClassifier(p_options=[2, 4], max_samples=0.5, max_features = 0.7)\n",
        "    >>> random_scm.fit(X_train, y_train)\n",
        "    >>> hyperparams = random_scm.get_hyperparams()\n",
        "    >>> importances = random_scm.features_importance()\n",
        "    >>> disagree = random_scm.classifiers_disagreement(X)\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
        "           databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
        "    .. [2] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
        "           Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 n_estimators=100,\n",
        "                 max_samples=0.5,\n",
        "                 max_features=0.5,\n",
        "                 max_rules=10,\n",
        "                 p_options=[1.0],\n",
        "                 model_type=\"conjunction\",\n",
        "                 n_jobs=None,\n",
        "                 min_cq_combination=False,\n",
        "                 min_cq_mu = 10e-3,\n",
        "                 random_state=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_samples = max_samples\n",
        "        self.max_features = max_features\n",
        "        self.max_rules = max_rules\n",
        "        self.p_options = p_options\n",
        "        self.model_type = model_type\n",
        "        self.n_jobs = n_jobs\n",
        "        self.min_cq_combination = min_cq_combination\n",
        "        self.min_cq_mu = min_cq_mu\n",
        "        self.random_state = random_state\n",
        "        self.labels_to_binary = {}\n",
        "        self.binary_to_labels = {}\n",
        "\n",
        "    def p_for_estimators(self):\n",
        "        \"\"\"Return the value of p for each estimator to fit.\"\"\"\n",
        "        options_len = len(self.p_options) # number of options\n",
        "        estims_with_same_p = self.n_estimators // options_len # nb of estimators to fit with the same p\n",
        "        p_of_estims = []\n",
        "        if options_len > 1:\n",
        "            for k in range(options_len - 1):\n",
        "                opt = self.p_options[k] # an option\n",
        "                p_of_estims = p_of_estims + ([opt] * estims_with_same_p) # estims_with_same_p estimators with p=opt\n",
        "        p_of_estims = p_of_estims + ([self.p_options[-1]] * (self.n_estimators - len(p_of_estims)))\n",
        "        return p_of_estims\n",
        "\n",
        "    def get_estimators(self):\n",
        "        \"\"\"Return the list of estimators of the classifier\"\"\"\n",
        "        if hasattr(self, 'estimators'):\n",
        "            return self.estimators\n",
        "        else:\n",
        "            return \"not defined (model not fitted)\"\n",
        "\n",
        "    def get_hyperparams(self):\n",
        "        \"\"\"Return the model hyperparameters\"\"\"\n",
        "        hyperparams = {\n",
        "            'n_estimators' : self.n_estimators, \n",
        "            'max_samples' : self.max_samples, \n",
        "            'max_features' : self.max_features, \n",
        "            'max_rules' : self.max_rules, \n",
        "            'p_options' : self.p_options, \n",
        "            'model_type' : self.model_type, \n",
        "            'random_state' : self.random_state\n",
        "        }\n",
        "        return hyperparams\n",
        "\n",
        "    def labels_conversion(self, labels_list):\n",
        "        \"\"\"\n",
        "        Return the equivalence between labels and binaries\n",
        "        \"\"\"\n",
        "        l = list(set(labels_list))\n",
        "        labels_dict = {c:idx for idx, c in enumerate(l)}\n",
        "        if len(l) < 2:\n",
        "            raise ValueError(\"Only 1 classe given to the model, needs 2\")\n",
        "        elif len(l) > 2:\n",
        "             raise ValueError(\"{} classes were given, multiclass prediction is not implemented\".format(len(l)))\n",
        "        return np.array(l), labels_dict\n",
        "\n",
        "\n",
        "    def fit(self, X, y, get_feature_importances=True, tiebreaker=None):\n",
        "        \"\"\"\n",
        "        Fit the model with the given data\n",
        "        \"\"\"\n",
        "        # Check if 2 classes are inputed and convert labels to binary labels\n",
        "        X, y = check_X_y(X, y)\n",
        "        self.classes_, self.labels_to_binary = self.labels_conversion(y)\n",
        "        self.binary_to_labels = {bin_label:str_label for str_label, bin_label in self.labels_to_binary.items()}\n",
        "        y = np.array([self.labels_to_binary[l] for l in y])\n",
        "\n",
        "        # Save the original number of features\n",
        "        self.n_features = X.shape[1]\n",
        "\n",
        "        self.estimators = []\n",
        "        self.estim_features = []\n",
        "        max_rules = self.max_rules\n",
        "        p_of_estims_ = self.p_for_estimators()\n",
        "        model_type = self.model_type\n",
        "\n",
        "        #seeds for reproductibility\n",
        "        random_state = self.random_state\n",
        "        random_state = check_random_state(random_state)\n",
        "        seeds = random_state.randint(MAX_INT, size=self.n_estimators)\n",
        "        self._seeds = seeds\n",
        "\n",
        "        pop_samples, pop_features = X.shape\n",
        "        max_samples, max_features = self.max_samples, self.max_features\n",
        "\n",
        "        # validate max_samples\n",
        "        if not isinstance(max_samples, numbers.Integral):\n",
        "            max_samples = int(max_samples * pop_samples)\n",
        "        if not (0 < max_samples <= pop_samples):\n",
        "            raise ValueError(\"max_samples must be in (0, n_samples)\")\n",
        "        # store validated integer row sampling values\n",
        "        self._max_samples = max_samples\n",
        "        self._pop_samples = pop_samples\n",
        "\n",
        "        # validate max_features\n",
        "        if isinstance(self.max_features, numbers.Integral):\n",
        "            max_features = self.max_features\n",
        "        elif isinstance(self.max_features, np.float):\n",
        "            max_features = self.max_features * pop_features\n",
        "        else:\n",
        "            raise ValueError(\"max_features must be int or float\")\n",
        "        if not (0 < max_features <= pop_features):\n",
        "            raise ValueError(\"max_features must be in (0, n_features)\")\n",
        "        max_features = max(1, int(max_features))\n",
        "        # store validated integer feature sampling values\n",
        "        self._max_features = max_features\n",
        "        self._pop_features = pop_features\n",
        "\n",
        "        # parallel loop\n",
        "        n_jobs, n_estimators_list, starts = _partition_estimators(self.n_estimators, self.n_jobs)\n",
        "        # building estimators\n",
        "        all_results = Parallel(n_jobs=n_jobs)(delayed(_parallel_build_estimators)(\n",
        "                range(starts[i],starts[i+1]), self, p_of_estims_, seeds, X, y, tiebreaker)\n",
        "            for i in range(n_jobs))\n",
        "\n",
        "        self.estimators += list(itertools.chain.from_iterable(\n",
        "            t[0] for t in all_results))\n",
        "        self.estim_features += list(itertools.chain.from_iterable(\n",
        "            t[1] for t in all_results))\n",
        "\n",
        "        if self.min_cq_combination:\n",
        "            predictions = np.zeros((X.shape[0], self.n_estimators))\n",
        "            for k in range(self.n_estimators):\n",
        "                predictions[:, k] = self.estimators[k].predict(X[:, self.estim_features[k]])\n",
        "\n",
        "            from .min_cq import MinCqLearner\n",
        "            mincq = MinCqLearner(self.min_cq_mu, \"stumps\", n_stumps_per_attribute=1,\n",
        "                                 self_complemented=False)\n",
        "            mincq.fit(predictions, y)\n",
        "            self.min_cq_weights = mincq.majority_vote.weights\n",
        "            self.min_cq_weights /= np.sum(np.abs(self.min_cq_weights))\n",
        "\n",
        "        if get_feature_importances:\n",
        "            importances = self.features_importance()\n",
        "            self.feature_importances_ = np.array([importances['avg'][k]\n",
        "                                                  if k in importances['avg'] else 0\n",
        "                                                  for k in range(self.n_features)])\n",
        "            self.feature_importances_ /= np.sum(self.feature_importances_)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Compute model predictions for data in X\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        predictions : array\n",
        "            predictions[i] is the predicted class for he sample i in X\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, [\"estimators\", \"estim_features\"])\n",
        "        X = check_array(X)\n",
        "        predicted_proba = self.predict_proba(X)\n",
        "        predictions = np.array(np.argmax(predicted_proba, axis=1), dtype=int)\n",
        "        predictions = np.array([self.binary_to_labels[l] for l in predictions])\n",
        "        return predictions\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict class probabilities according to the model estimators\n",
        "\n",
        "        Parameters :\n",
        "        ----------\n",
        "        X : array\n",
        "            a dataset to predict\n",
        "\n",
        "        Returns : \n",
        "        ----------\n",
        "        proba : array\n",
        "            proba[c] contains the estimated probability for each sample of X to belong to class c\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, [\"estimators\", \"estim_features\"])\n",
        "        X = check_array(X)\n",
        "        # parallel loop\n",
        "        n_jobs, n_estimators_list, starts = _partition_estimators(self.n_estimators, self.n_jobs)\n",
        "        results = np.zeros(X.shape[0])\n",
        "        results = Parallel(n_jobs=n_jobs)(delayed(_parallel_predict_proba)(\n",
        "                self, X, range(starts[i], starts[i+1]), results)\n",
        "                for i in range(n_jobs))\n",
        "        if not self.min_cq_combination:\n",
        "            votes = sum(results) / self.n_estimators\n",
        "            proba = np.array([np.array([1 - vote, vote]) for vote in votes])\n",
        "        else:\n",
        "            votes = sum(results)\n",
        "            proba = np.array([np.array([(1 - vote)/2, (1 + vote)/2]) for vote in votes])\n",
        "        return proba\n",
        "\n",
        "    def features_importance(self):\n",
        "        \"\"\"\n",
        "        Compute features importances in estimators rules\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        importance : dict\n",
        "            importances['avg'] : dict (feature id as key, mean importance as value)\n",
        "                The mean importance of each feature over the estimators.\n",
        "            importances['max'] : dict (feature id as key, max importance as value)\n",
        "                The maximal importance of each feature over the estimators.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, [\"estimators\", \"estim_features\"])\n",
        "        importances = {'avg' : {}, 'max' : {}} # average and maximal feature/rule importances\n",
        "        feature_id_occurences = {} # number of occurences of a feature in subsamples\n",
        "        for (estim, features_idx) in zip(self.estimators, self.estim_features):\n",
        "            # increment the total occurences of the feature :\n",
        "            for id_feat in features_idx:\n",
        "                if id_feat in feature_id_occurences:\n",
        "                    feature_id_occurences[id_feat] += 1\n",
        "                else:\n",
        "                    feature_id_occurences[id_feat] = 1\n",
        "            # sum the rules importances :\n",
        "            #rules_importances = estim.get_rules_importances() # activate it when pyscm will implement importance\n",
        "            rules_importances = np.ones(len(estim.model_.rules)) #delete it when pyscm will implement importance\n",
        "            for rule, importance in zip(estim.model_.rules, rules_importances):\n",
        "                global_feat_id = features_idx[rule.feature_idx]\n",
        "                if global_feat_id in importances['avg']:\n",
        "                    importances['avg'][global_feat_id] += importance\n",
        "                    if importance > importances['max'][global_feat_id]:\n",
        "                        importances['max'][global_feat_id] = importance\n",
        "                else:\n",
        "                    importances['avg'][global_feat_id] = importance\n",
        "                    importances['max'][global_feat_id] = importance\n",
        "        importances['avg'] = {k: round(v / feature_id_occurences[k], 3) for k, v in importances['avg'].items()}\n",
        "        return importances\n",
        "\n",
        "    def all_data_tiebreaker(self, model_type, feature_idx, thresholds, rule_type, X, y):\n",
        "        \"\"\"\n",
        "        Choose a rule between rule with equal utility\n",
        "        Select the one that have the best accuracy if applied alone on all the data\n",
        "\n",
        "        Parameters :\n",
        "        ----------\n",
        "        model_type : strint ('conjunction' or 'dijunction')\n",
        "            type of the model\n",
        "        feature_idx, thresholds, rule_type : arrays\n",
        "            description of the rules\n",
        "        X, y : arrays\n",
        "            a dataset used to compare rules\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        ID of the rule to select\n",
        "        \"\"\"\n",
        "        keep_id = 0\n",
        "        keep_id_score = -1\n",
        "        for k in range(len(feature_idx)):\n",
        "            feat_id, threshold, r_type = feature_idx[k], thresholds[k], rule_type[k]\n",
        "            stump = DecisionStump(feature_idx=feat_id, threshold=threshold, kind=r_type)\n",
        "            rule_classif = stump.classify(X).astype('int')\n",
        "            rule_global_score = (rule_classif == y).sum()\n",
        "            if rule_global_score > keep_id_score:\n",
        "                keep_id = k\n",
        "                keep_id_score = rule_global_score\n",
        "        return keep_id\n",
        "\n",
        "    def classifiers_disagreement(self, X):\n",
        "        \"\"\"\n",
        "        Compute disagreement between estimators\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        disagreement : dict \n",
        "            disagreement['global'] : int\n",
        "                global disagreement, computed with the following formula :\n",
        "                average(disagreement(c1, c2))\n",
        "                for (c1, c2) all the pairs of distinct estimators in the model\n",
        "                with \n",
        "                disagreement(c1, c2) = (# of examples where c1 and c2 differs)/(# of examples)\n",
        "            disagreement['estims'] : list\n",
        "                list of disagreements of each estimator with the global model\n",
        "            disagreement['heatmap'] : matrix\n",
        "                heatmap of mutual disagreement for each pair of classifiers\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, [\"estimators\", \"estim_features\"])\n",
        "        disagreement = {'global' : 0, 'estims' : []}\n",
        "        total_comp = 0 # number of comparison of estimators\n",
        "        global_pred = self.predict(X)\n",
        "        heatmap = np.zeros((self.n_estimators, self.n_estimators))\n",
        "        for kA in range(len(self.estimators)):\n",
        "            estimA = self.estimators[kA]\n",
        "            predA = estimA.predict(X[:, self.estim_features[kA]])\n",
        "            disagree = np.not_equal(global_pred, predA).sum()/len(X) # disagreement of estimator A and global model\n",
        "            disagreement['estims'].append(disagree)\n",
        "            for kB in range(len(self.estimators)):\n",
        "                if kB == kA:\n",
        "                    continue # do not compare the estimator with itself\n",
        "                estimB = self.estimators[kB]\n",
        "                predB = estimB.predict(X[:, self.estim_features[kB]])\n",
        "                disagree = np.not_equal(predB, predA).sum()/len(X) # disagreement of estimators A and B\n",
        "                heatmap[kA][kB] = disagree\n",
        "                disagreement['global'] += disagree\n",
        "                total_comp += 1\n",
        "        disagreement['global'] = disagreement['global']/total_comp\n",
        "        disagreement['heatmap'] = heatmap\n",
        "        return disagreement\n",
        "    \n",
        "    def get_estimators_indices(self):\n",
        "        \"\"\"\n",
        "        Get drawn indices along both sample and feature axes\n",
        "        \"\"\"\n",
        "        for seed in self._seeds:\n",
        "            # operations accessing random_state must be performed identically to those in 'fit'\n",
        "            feature_indices = sample_without_replacement(self._pop_features, self._max_features, random_state=seed)\n",
        "            samples_indices = sample_without_replacement(self._pop_samples, self._max_samples, random_state=seed)\n",
        "            yield samples_indices\n",
        "\n",
        "    def score(self, X, y):\n",
        "        check_is_fitted(self, [\"estimators\", \"estim_features\"])\n",
        "        X, y = check_X_y(X, y)\n",
        "        return accuracy_score(y, self.predict(X))"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiePDO-Q1HP-",
        "outputId": "48cfe310-ab57-406d-9fad-b4f6631a4317"
      },
      "source": [
        "### Ici on instancie la classe RandomScmClassifier() et on vérifie les paramètres par défaut\n",
        "result = RandomScmClassifier()\n",
        "from pprint import pprint\n",
        "# Look at parameters used by our current forest\n",
        "print('Parameters currently in use:\\n')\n",
        "pprint(result.get_params())"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameters currently in use:\n",
            "\n",
            "{'max_features': 0.5,\n",
            " 'max_rules': 10,\n",
            " 'max_samples': 0.5,\n",
            " 'min_cq_combination': False,\n",
            " 'min_cq_mu': 0.01,\n",
            " 'model_type': 'conjunction',\n",
            " 'n_estimators': 100,\n",
            " 'n_jobs': None,\n",
            " 'p_options': [1.0],\n",
            " 'random_state': None}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPwYlBCz22NR"
      },
      "source": [
        "### Ici on divise les données avec un train/test de 80/20 et on rajoute un seed de 42\n",
        "from sklearn.model_selection import train_test_split \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq6KAnXN3J4D"
      },
      "source": [
        "### Ici on entraine le modèle SCM avec les paramètres par défaut:\n",
        "result.fit(X_train, y_train)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBgSqo8d3Wyw",
        "outputId": "5979453f-999a-4674-d13b-5ad7b3a8ef20"
      },
      "source": [
        "### Ici on calcule les prédictions avec le X_test puis on calcule la Test Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "predictions = result.predict(X_train)\n",
        "print(\"Train Accuracy:\", metrics.accuracy_score(y_train, predictions))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy: 0.7875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qpuhH-ZDroV",
        "outputId": "19d9d7fa-2c01-4fbc-9600-b6a6e0254ec0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### Ici on calcule les prédictions avec le X_test puis on calcule la Test Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "predictions = result.predict(X_test)\n",
        "print(\"Test Accuracy:\", metrics.accuracy_score(y_test, predictions))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ba1P99V73wJX",
        "outputId": "d273f2ba-e18b-424e-9796-778a7a5ed68b"
      },
      "source": [
        "### Ici, vérification avec f1_score\n",
        "from sklearn.metrics import f1_score\n",
        "f1_score(y_test, predictions)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6666666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raY15kVxGayQ",
        "outputId": "09c30dc1-a5ce-4ab3-8da6-011782f1ca4e"
      },
      "source": [
        "### Ici on reentraine le modèle avec du Boostrapping 60 fois\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn import metrics\n",
        "#### Bootstrapping ####\n",
        "########################################################\n",
        "# Creating empty list to hold accuracy values\n",
        "AccuracyValues=[]\n",
        "n_times=60\n",
        "## Performing bootstrapping\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split \n",
        "for i in range(n_times):\n",
        "    #Split the data into training and testing set\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    # Changing the seed value for each iteration\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42+i)\n",
        "    result = RandomScmClassifier(max_features = 0.5, max_samples = 0.5, model_type = 'disjunction', n_estimators = 10, p_options = [5.0])\n",
        "    result.fit(X_train, y_train)\n",
        "    ########################################################\n",
        "    prediction = result.predict(X_test)\n",
        "    Accuracy=metrics.accuracy_score(y_test, prediction)\n",
        "    print(Accuracy)\n",
        "    AccuracyValues.append((Accuracy))\n",
        "    #print(Accuracy)\n",
        "    print(AccuracyValues)\n",
        "     #Measuring accuracy on Testing Data\n",
        "#Accuracy=100- (np.mean(np.abs((y_test - prediction) / y_test)) * 100)\n",
        "    \n",
        "    # Storing accuracy values\n",
        "#print(AccuracyValues.append(np.round(Accuracy)))\n",
        "    \n",
        "################################################\n",
        "# Result of all bootstrapping trials\n",
        "#print(AccuracyValues)\n",
        " \n",
        "# Final accuracy\n",
        "print('Final average accuracy',np.mean(AccuracyValues))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5\n",
            "[0.5]\n",
            "0.3\n",
            "[0.5, 0.3]\n",
            "0.55\n",
            "[0.5, 0.3, 0.55]\n",
            "0.45\n",
            "[0.5, 0.3, 0.55, 0.45]\n",
            "0.55\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55]\n",
            "0.65\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65]\n",
            "0.45\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45]\n",
            "0.65\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65]\n",
            "0.7\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7]\n",
            "0.5\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5]\n",
            "0.35\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35]\n",
            "0.6\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6]\n",
            "0.4\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4]\n",
            "0.55\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55]\n",
            "0.6\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6]\n",
            "0.75\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75]\n",
            "0.4\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4]\n",
            "0.6\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6]\n",
            "0.5\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5]\n",
            "0.5\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5]\n",
            "0.45\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45]\n",
            "0.45\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45]\n",
            "0.5\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5]\n",
            "0.55\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55]\n",
            "0.55\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55]\n",
            "0.45\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45]\n",
            "0.65\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65]\n",
            "0.8\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8]\n",
            "0.4\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4]\n",
            "0.5\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5]\n",
            "0.55\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55]\n",
            "0.55\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55]\n",
            "0.65\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65]\n",
            "0.45\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45]\n",
            "0.7\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7]\n",
            "0.45\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45]\n",
            "0.6\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6]\n",
            "0.45\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45]\n",
            "0.4\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4]\n",
            "0.6\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4, 0.6]\n",
            "0.4\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4, 0.6, 0.4]\n",
            "0.4\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4, 0.6, 0.4, 0.4]\n",
            "0.45\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4, 0.6, 0.4, 0.4, 0.45]\n",
            "0.55\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4, 0.6, 0.4, 0.4, 0.45, 0.55]\n",
            "0.55\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4, 0.6, 0.4, 0.4, 0.45, 0.55, 0.55]\n",
            "0.45\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4, 0.6, 0.4, 0.4, 0.45, 0.55, 0.55, 0.45]\n",
            "0.4\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4, 0.6, 0.4, 0.4, 0.45, 0.55, 0.55, 0.45, 0.4]\n",
            "0.5\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4, 0.6, 0.4, 0.4, 0.45, 0.55, 0.55, 0.45, 0.4, 0.5]\n",
            "0.5\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4, 0.6, 0.4, 0.4, 0.45, 0.55, 0.55, 0.45, 0.4, 0.5, 0.5]\n",
            "0.7\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4, 0.6, 0.4, 0.4, 0.45, 0.55, 0.55, 0.45, 0.4, 0.5, 0.5, 0.7]\n",
            "0.6\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4, 0.6, 0.4, 0.4, 0.45, 0.55, 0.55, 0.45, 0.4, 0.5, 0.5, 0.7, 0.6]\n",
            "0.55\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4, 0.6, 0.4, 0.4, 0.45, 0.55, 0.55, 0.45, 0.4, 0.5, 0.5, 0.7, 0.6, 0.55]\n",
            "0.6\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4, 0.6, 0.4, 0.4, 0.45, 0.55, 0.55, 0.45, 0.4, 0.5, 0.5, 0.7, 0.6, 0.55, 0.6]\n",
            "0.65\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4, 0.6, 0.4, 0.4, 0.45, 0.55, 0.55, 0.45, 0.4, 0.5, 0.5, 0.7, 0.6, 0.55, 0.6, 0.65]\n",
            "0.65\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4, 0.6, 0.4, 0.4, 0.45, 0.55, 0.55, 0.45, 0.4, 0.5, 0.5, 0.7, 0.6, 0.55, 0.6, 0.65, 0.65]\n",
            "0.55\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4, 0.6, 0.4, 0.4, 0.45, 0.55, 0.55, 0.45, 0.4, 0.5, 0.5, 0.7, 0.6, 0.55, 0.6, 0.65, 0.65, 0.55]\n",
            "0.65\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4, 0.6, 0.4, 0.4, 0.45, 0.55, 0.55, 0.45, 0.4, 0.5, 0.5, 0.7, 0.6, 0.55, 0.6, 0.65, 0.65, 0.55, 0.65]\n",
            "0.5\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4, 0.6, 0.4, 0.4, 0.45, 0.55, 0.55, 0.45, 0.4, 0.5, 0.5, 0.7, 0.6, 0.55, 0.6, 0.65, 0.65, 0.55, 0.65, 0.5]\n",
            "0.5\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4, 0.6, 0.4, 0.4, 0.45, 0.55, 0.55, 0.45, 0.4, 0.5, 0.5, 0.7, 0.6, 0.55, 0.6, 0.65, 0.65, 0.55, 0.65, 0.5, 0.5]\n",
            "0.6\n",
            "[0.5, 0.3, 0.55, 0.45, 0.55, 0.65, 0.45, 0.65, 0.7, 0.5, 0.35, 0.6, 0.4, 0.55, 0.6, 0.75, 0.4, 0.6, 0.5, 0.5, 0.45, 0.45, 0.5, 0.55, 0.55, 0.45, 0.65, 0.8, 0.4, 0.5, 0.55, 0.55, 0.65, 0.45, 0.7, 0.45, 0.6, 0.45, 0.4, 0.6, 0.4, 0.4, 0.45, 0.55, 0.55, 0.45, 0.4, 0.5, 0.5, 0.7, 0.6, 0.55, 0.6, 0.65, 0.65, 0.55, 0.65, 0.5, 0.5, 0.6]\n",
            "Final average accuracy 0.5333333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JW35_EWE4Fgv",
        "outputId": "70109b3f-dd3a-4302-9d17-d1efa40060fd"
      },
      "source": [
        "### Ici on commence la Cross Validation pour trouver la meilleure combinaison de paramètres\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# The number of base estimators in the ensemble.\n",
        "n_estimators = [1, 10, 100, 1000] \n",
        "# Number of maximum features The number of features to draw from X to train each base estimator (without replacement.)\n",
        "max_features = [0.25, 0.5, 0.75, 1.0] \n",
        "# Maximum samples\n",
        "max_samples = [0.25, 0.5, 0.75, 1.0]\n",
        "# p_options: The estimators will be fitted with values of p found in p_options \n",
        "p_options = [[0.1], [0.5], [1.0], [5.0], [10.0]]\n",
        "# model_type: type of estimators to build\n",
        "model_type = ['conjunction', 'disjunction']\n",
        "# bootstrap\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_samples': max_samples,\n",
        "               'p_options': p_options,\n",
        "               'model_type': model_type}\n",
        "pprint(random_grid)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'max_features': [0.25, 0.5, 0.75, 1.0],\n",
            " 'max_samples': [0.25, 0.5, 0.75, 1.0],\n",
            " 'model_type': ['conjunction', 'disjunction'],\n",
            " 'n_estimators': [1, 10, 100, 1000],\n",
            " 'p_options': [[0.1], [0.5], [1.0], [5.0], [10.0]]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5004T1qmFLFz",
        "outputId": "129b6316-90da-4f24-d9c1-e66900780bbc"
      },
      "source": [
        "### Ici on entraîne les différentes combinaisons de paramètres\n",
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "#rf = RandomForestClassifier()\n",
        "CVresult = RandomScmClassifier()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "CVresult = RandomizedSearchCV(estimator = CVresult, param_distributions = random_grid, n_iter = 200, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "CVresult.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   19.0s\n",
            "[Parallel(n_jobs=-1)]: Done 164 tasks      | elapsed:  1.8min\n",
            "[Parallel(n_jobs=-1)]: Done 367 tasks      | elapsed:  3.5min\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMlSWrYQM42n",
        "outputId": "039d7486-4877-456b-eb45-90382d359294"
      },
      "source": [
        "CVresult.best_params_   ### Meilleurs paramètres obtenus en Cross-Validation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_features': 1.0,\n",
              " 'max_samples': 0.5,\n",
              " 'model_type': 'disjunction',\n",
              " 'n_estimators': 100,\n",
              " 'p_options': [5.0]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhjS_rhYKPhj"
      },
      "source": [
        "### Nouvelle instantiation du RandomSCM avec les paramètres trouvés en Cross-Validation\n",
        "CVresult = RandomScmClassifier(max_features = 1.0, max_samples = 0.5, model_type = 'disjunction', n_estimators = 100, p_options = [5.0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3hK75P4Lkpv"
      },
      "source": [
        "### entraînement du modèle avec les paramètres trouvés en Cross-Validation\n",
        "from sklearn.model_selection import train_test_split \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "CVresult.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hoakwdWLxQ1",
        "outputId": "c34b20b4-f83a-485e-c2c6-09ad58b724ab"
      },
      "source": [
        "### Calcul de y_test et de l'accuracy \n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "predictions = CVresult.predict(X_test)\n",
        "print(\"Test Accuracy:\", metrics.accuracy_score(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZeXvOioL8rb",
        "outputId": "5ad3891a-0c84-4293-f3e4-b22ba463bb24"
      },
      "source": [
        "### Vérification avec f1_score\n",
        "from sklearn.metrics import f1_score\n",
        "f1_score(y_test, predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6363636363636364"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hamyJHxtMDU1",
        "outputId": "2d737b72-309d-4473-ed00-90ce61f64978"
      },
      "source": [
        "### Boostraping 60 fois avec Paramètres obtenus en Cross-Validation\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn import metrics\n",
        "#### Bootstrapping ####\n",
        "########################################################\n",
        "# Creating empty list to hold accuracy values\n",
        "AccuracyValues=[]\n",
        "n_times=60\n",
        "## Performing bootstrapping\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split \n",
        "for i in range(n_times):\n",
        "    #Split the data into training and testing set\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    # Changing the seed value for each iteration\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42+i)\n",
        "    CVresult = RandomScmClassifier(max_features = 1.0, max_samples = 0.5, model_type = 'disjunction', n_estimators = 100, p_options = [5.0])\n",
        "    CVresult.fit(X_train, y_train)\n",
        "    ########################################################\n",
        "    prediction = CVresult.predict(X_test)\n",
        "    Accuracy=metrics.accuracy_score(y_test, prediction)\n",
        "    print(Accuracy)\n",
        "    AccuracyValues.append((Accuracy))\n",
        "    #print(Accuracy)\n",
        "    print(AccuracyValues)\n",
        "    \n",
        "    #Creating the model on Training Data\n",
        "    #DTree=RegModel.fit(X_train,y_train)\n",
        "    #prediction=DTree.predict(X_test)\n",
        " \n",
        "    #Measuring accuracy on Testing Data\n",
        "#Accuracy=100- (np.mean(np.abs((y_test - prediction) / y_test)) * 100)\n",
        "    \n",
        "    # Storing accuracy values\n",
        "#AccuracyValues.append(np.round(Accuracy))\n",
        "    \n",
        "################################################\n",
        "# Result of all bootstrapping trials\n",
        "print(AccuracyValues)\n",
        " \n",
        "# Final accuracy\n",
        "print('Final average accuracy',np.mean(AccuracyValues))\n",
        "#print(\"Test Accuracy:\", metrics.accuracy_score(y_test, y_final ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6\n",
            "[0.6]\n",
            "0.6\n",
            "[0.6, 0.6]\n",
            "0.4\n",
            "[0.6, 0.6, 0.4]\n",
            "0.7\n",
            "[0.6, 0.6, 0.4, 0.7]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6]\n",
            "0.7\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6]\n",
            "0.55\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55]\n",
            "0.35\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5]\n",
            "0.7\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6]\n",
            "0.75\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75]\n",
            "0.65\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65]\n",
            "0.65\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5]\n",
            "0.45\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45]\n",
            "0.55\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55]\n",
            "0.65\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6]\n",
            "0.7\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5]\n",
            "0.45\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45]\n",
            "0.7\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7]\n",
            "0.4\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4]\n",
            "0.45\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45]\n",
            "0.55\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55]\n",
            "0.45\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45]\n",
            "0.55\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55]\n",
            "0.65\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6]\n",
            "0.55\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5]\n",
            "0.55\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5]\n",
            "0.4\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4]\n",
            "0.65\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6]\n",
            "0.55\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55]\n",
            "0.55\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55]\n",
            "0.45\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45]\n",
            "0.4\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4]\n",
            "0.7\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7, 0.6]\n",
            "0.65\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7, 0.6, 0.65]\n",
            "0.45\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7, 0.6, 0.65, 0.45]\n",
            "0.45\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7, 0.6, 0.65, 0.45, 0.45]\n",
            "0.7\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7, 0.6, 0.65, 0.45, 0.45, 0.7]\n",
            "0.6\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7, 0.6, 0.65, 0.45, 0.45, 0.7, 0.6]\n",
            "0.45\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7, 0.6, 0.65, 0.45, 0.45, 0.7, 0.6, 0.45]\n",
            "0.55\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7, 0.6, 0.65, 0.45, 0.45, 0.7, 0.6, 0.45, 0.55]\n",
            "0.5\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7, 0.6, 0.65, 0.45, 0.45, 0.7, 0.6, 0.45, 0.55, 0.5]\n",
            "[0.6, 0.6, 0.4, 0.7, 0.6, 0.7, 0.6, 0.5, 0.6, 0.55, 0.35, 0.5, 0.7, 0.6, 0.75, 0.65, 0.65, 0.5, 0.45, 0.55, 0.65, 0.6, 0.7, 0.5, 0.5, 0.5, 0.45, 0.7, 0.4, 0.45, 0.55, 0.45, 0.55, 0.65, 0.6, 0.55, 0.5, 0.5, 0.5, 0.55, 0.6, 0.5, 0.4, 0.65, 0.6, 0.6, 0.55, 0.55, 0.45, 0.4, 0.7, 0.6, 0.65, 0.45, 0.45, 0.7, 0.6, 0.45, 0.55, 0.5]\n",
            "Final average accuracy 0.5558333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}